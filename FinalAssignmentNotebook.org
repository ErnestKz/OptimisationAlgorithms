#+AUTHOR:Ernests Kuznecovs - 17332791 - kuznecoe@tcd.ie
#+Date:2021-2022
#+Title:Optimisation Algorithms - Final Assignment

#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
    }
\lstset{style=mystyle}
#+end_export

* Preamble                                                         :noexport:
#+PROPERTY: header-args:python :session fa
#+PROPERTY: header-args:python+ :async yes
#+PROPERTY: header-args:python+ :eval never-export
#+PROPERTY: header-args:elisp :eval never-export
#+EXCLUDE_TAGS: noexport
#+STARTUP: overview
#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage[a4paper, total={6.7in, 10.5in}]{geometry}

#+LaTeX_HEADER: \usepackage{caption}
#+LaTeX_HEADER: \newcommand\figwidth{0.48}

#+begin_src elisp :results none :exports none
(setq-local org-image-actual-width '(512))
(setq-local org-confirm-babel-evaluate nil)
(setq-local org-src-preserve-indentation 't)

(setq org-latex-listings t)
(setq org-latex-prefer-user-labels t)
#+end_src

#+begin_src elisp :results none :exports none
(use-package jupyter
  :config
  (org-babel-do-load-languages 'org-babel-load-languages '((emacs-lisp . t)
							   (python . t)
							   (jupyter . t)))
  (org-babel-jupyter-override-src-block "python")
  (add-hook 'org-babel-after-execute-hook 'org-redisplay-inline-images)
  (org-babel-do-load-languages
   'org-babel-load-languages
   '((emacs-lisp . t)
     (python . t)
     (jupyter . t))))
#+end_src

* Python Imports                                                   :noexport:

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 200
mpl.rcParams['figure.facecolor'] = '1'
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')

import copy
import numpy as np
from sklearn import metrics
#+end_src

* Flax, Jax, Optax Examples                                        :noexport:

** Flax
#+begin_src python :results none :exports none :tangle ./FinalSrc.py
from typing import Sequence

import numpy as np
import jax
import jax.numpy as jnp
import flax.linen as nn

class MLP(nn.Module):
  features: Sequence[int]

  @nn.compact
  def __call__(self, x):
    for feat in self.features[:-1]:
      x = nn.relu(nn.Dense(feat)(x))
    x = nn.Dense(self.features[-1])(x)
    return x

model = MLP([12, 8, 4])
batch = jnp.ones((32, 10))
variables = model.init(jax.random.PRNGKey(0), batch)
output = model.apply(variables, batch)
#+end_src

** Optax

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
import random
from typing import Tuple

import optax
import jax.numpy as jnp
import jax
import numpy as np

BATCH_SIZE = 5
NUM_TRAIN_STEPS = 1_000
RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))

TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)
LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
initial_params = {
    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),
    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),
}


def net(x: jnp.ndarray, params: jnp.ndarray) -> jnp.ndarray:
  x = jnp.dot(x, params['hidden'])
  x = jax.nn.relu(x)
  x = jnp.dot(x, params['output'])
  return x


def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:
  y_hat = net(batch, params)

  # optax also provides a number of common loss functions.
  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)

  return loss_value.mean()
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:
  opt_state = optimizer.init(params)

  @jax.jit
  def step(params, opt_state, batch, labels):
    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)
    updates, opt_state = optimizer.update(grads, opt_state, params)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss_value

  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):
    params, opt_state, loss_value = step(params, opt_state, batch, labels)
    if i % 100 == 0:
      print(f'step {i}, loss: {loss_value}')

  return params

# Finally, we can fit our parametrized function using the Adam optimizer
# provided by optax.
optimizer = optax.adam(learning_rate=1e-2)
optimizer2 = optax.sgd(learning_rate=1e-2)
params = fit(initial_params, optimizer)
params = fit(initial_params, optimizer2)
#+end_src

* Assignment                                                       :noexport:

- Need to complete declaration.
- Include code as text.
- Porgrams should be running code.
- Reports should be 5 pages, 10 pages upper limit


- Comparing performance of SGD with
  - Adam
  - Constant Step size

    
- To do this need to make important choices.

  
  - How to measure performance.
    - e.g plot ML loss function vs optimisation iterations
      - use lowest value as performance measure
	- but this measures performance on training data, not on unseen (non-generalised)
    - e.g measure ML loss function on held-out test data
    - good idea to look at both measures

  - SGD involves randomisation
    - may be necessary to collect data from several runs
      - to understand how performance fluctuates from run to run

  - What hyperparameters to use and how to choose them.
    - Look at performance of both when using
      - default hyperparameter values
      - and when using optimised values (global random search?)

  - What ML model and data to use for evaluation.
    - probably worth 2 models/datasets
    - at least 1 neural net ML model
    - MNIST, CIFAR, Imbd

  - Existing examples of performance evaluation
    - Adam: A Method For Stochastic Optimization
      - https://arxiv.org/pdf/1412.6980.pdf
      - Training error vs other algorithms
    - The Marginal Value of Adaptive Gradient Methods in Machine Learning
      - https://arxiv.org/pdf/1705.08292.pdf
      - Test error (i.e generalisaton)
      - of SGD against a range of algorithms, including Adam

    - Might reflect on, do these papers address choices noted above?
      - if not, might it be important or not?

* Libraries, Documentation, Resources                              :noexport:
** Optax - Optimisation Algorithms Library for Jax
- https://optax.readthedocs.io/en/latest/api.html#sgd
- https://optax.readthedocs.io/en/latest/api.html#adam
- https://optax.readthedocs.io/en/latest/

** Flax - Neural Network Library for Jax
- AiEpiphany
  - https://www.youtube.com/watch?v=5eUSmJvK8WA&t=13s
  - https://github.com/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_4_Flax_Zero2Hero_Colab.ipynb
    
- https://github.com/google/flax
  - Can use MNIST, CIFAR10 example
  
** Jax
- https://colinraffel.com/blog/you-don-t-know-jax.html
- AiEpiphany
  - Part 1 - https://www.youtube.com/watch?v=SstuvS-tVc0&t=1649s
  - Part 2 - https://www.youtube.com/watch?v=CQQaifxuFcs&t=62s
  - Part 3 - https://www.youtube.com/watch?v=6_PqUPxRmjY&t=1155s

* Tasks                                                            :noexport:
- Find one more model (text analytics one?)
  
- Run multiple times to get list of loss histories
  - Then compute error bars of the loss histories

* Notes on Flax, Jax, Optax
- SGD implemented by chaining in Optax

* Tests and Evaluation                                             :noexport:
** Characteristics of the Domain
- Constant vs Adam

- Both have randomness.  
- Both have batch size.

- Probably constant, common epochs.

- Constant:
  - Alpha 
- Adam:
  - Alpha
  - Beta1
  - Beta2

- Default params for Adam:
  - Alpha = 0.001
  - Beta1 = 0.9
  - Beta2 = 0.999
- Take 0.01 as default for Constant.


- Then picking good hyperparameter values.
  - Global Random Search
    
** Evaluations and Visualisation

- Can't really have contour plot without quite a bit of effort.
  - Would have to look at 2 parameters at a time.
    - Perhaps see how countour of 2 parameters change over time, as other parameters are changed.

- With default hyperparams:

  - Plot:
    - Loss Function vs Optimisation Iteration (With error bars perhaps)

  - Boxplot:
    - Lowest value of loss function as performance measure. (non-generalised)
    - ML performance on held out data as performance measure. (generalised)

- With optimised hyperparams (Global Random Search):
  
  - Same stuff.

- Data for Plot ad Boxplot can be gethered in the same runs.

* Datasets and Models                                              :noexport:
- https://github.com/google/flax/tree/main/examples/lm1b
- https://github.com/google/flax/tree/main/examples/mnist
- https://github.com/google/flax/tree/main/examples/sst2

** mnist

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
from absl import logging
from flax import linen as nn
from flax.metrics import tensorboard
from flax.training import train_state
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
import optax
import tensorflow_datasets as tfds
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
class CNN(nn.Module):
  """A simple CNN model."""

  @nn.compact
  def __call__(self, x):
    x = nn.Conv(features=32, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = nn.Conv(features=64, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = x.reshape((x.shape[0], -1))  # flatten
    x = nn.Dense(features=256)(x)
    x = nn.relu(x)
    x = nn.Dense(features=10)(x)
    return x
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
@jax.jit
def apply_model(state, images, labels):
  """Computes gradients, loss and accuracy for a single batch."""
  def loss_fn(params):
    logits = CNN().apply({'params': params}, images)
    one_hot = jax.nn.one_hot(labels, 10)
    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))
    return loss, logits

  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
  (loss, logits), grads = grad_fn(state.params)
  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  return grads, loss, accuracy

@jax.jit
def update_model(state, grads):
  return state.apply_gradients(grads=grads)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def train_epoch(state, train_ds, batch_size, rng, loss_history):
  """Train for a single epoch."""
  train_ds_size = len(train_ds['image'])
  steps_per_epoch = train_ds_size // batch_size

  perms = jax.random.permutation(rng, len(train_ds['image']))
  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch
  perms = perms.reshape((steps_per_epoch, batch_size))

  epoch_loss = []
  epoch_accuracy = []

  for perm in perms:
    batch_images = train_ds['image'][perm, ...]
    batch_labels = train_ds['label'][perm, ...]
    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)
    state = update_model(state, grads)
    epoch_loss.append(loss)
    loss_history.append(loss)
    epoch_accuracy.append(accuracy)

  train_loss = np.mean(epoch_loss)
  train_accuracy = np.mean(epoch_accuracy)
  return state, train_loss, train_accuracy
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def get_datasets():
  """Load MNIST train and test datasets into memory."""
  ds_builder = tfds.builder('mnist')
  ds_builder.download_and_prepare()
  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))
  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))
  train_ds['image'] = jnp.float32(train_ds['image']) / 255.
  test_ds['image'] = jnp.float32(test_ds['image']) / 255.
  return train_ds, test_ds
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def create_train_state(rng, config):
  """Creates initial `TrainState`."""
  cnn = CNN()
  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']

  tx = config.optimiser
  
  return train_state.TrainState.create(
      apply_fn=cnn.apply, params=params, tx=tx)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def train_and_evaluate(config: ml_collections.ConfigDict,
                       workdir: str) -> train_state.TrainState:
  """Execute model training and evaluation loop.
  Args:
    config: Hyperparameter configuration for training and evaluation.
    workdir: Directory where the tensorboard summaries are written to.
  Returns:
    The train state (which includes the `.params`).
  """
  train_ds, test_ds = get_datasets()
  rng = jax.random.PRNGKey(0)

  summary_writer = tensorboard.SummaryWriter(workdir)
  summary_writer.hparams(dict(config))

  rng, init_rng = jax.random.split(rng)
  state = create_train_state(init_rng, config)

  for epoch in range(1, config.num_epochs + 1):
    rng, input_rng = jax.random.split(rng)
    state, train_loss, train_accuracy = train_epoch(state, train_ds,
                                                    config.batch_size,
                                                    input_rng)
    _, test_loss, test_accuracy = apply_model(state, test_ds['image'],
                                              test_ds['label'])

    logging.info(
        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'
        % (epoch, train_loss, train_accuracy * 100, test_loss,
           test_accuracy * 100))

    summary_writer.scalar('train_loss', train_loss, epoch)
    summary_writer.scalar('train_accuracy', train_accuracy, epoch)
    summary_writer.scalar('test_loss', test_loss, epoch)
    summary_writer.scalar('test_accuracy', test_accuracy, epoch)
  summary_writer.flush()
  return state
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def train_and_evaluate(config: ml_collections.ConfigDict,
                       workdir: str,
                       train_ds,
                       test_ds):
  rng = jax.random.PRNGKey(0)
  rng, init_rng = jax.random.split(rng)
  state = create_train_state(init_rng, config)
  
  _, test_loss, test_accuracy = apply_model(state, test_ds['image'], test_ds['label'])
  # print('epoch:% 3d, test_loss: %.4f, test_accuracy: %.2f'
  #         % (0, test_loss, test_accuracy * 100))


  loss_history = []
  
  for epoch in range(1, config.num_epochs + 1):
    rng, input_rng = jax.random.split(rng)
    state, train_loss, train_accuracy = train_epoch(state, train_ds, config.batch_size, input_rng, loss_history)
    _, test_loss, test_accuracy = apply_model(state, test_ds['image'], test_ds['label'])

    print('epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'
          % (epoch, train_loss, train_accuracy * 100, test_loss, test_accuracy * 100))
  return state, loss_history, test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def get_config(opt, batch_size):
  """Get the default hyperparameter configuration."""
  config = ml_collections.ConfigDict()
  config.optimiser = opt
  config.batch_size = batch_size
  config.num_epochs = 1
  return config
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
train_ds, test_ds = get_datasets()
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def f(learning_rate, b1, b2, batch_size):
    opt = optax.adam(learning_rate=learning_rate, b1=b1, b2=b2)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, _, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
    return test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def f2(learning_rate, batch_size):
    opt = optax.sgd(learning_rate=learning_rate)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, _, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
    return test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def global_random_search(intervals, N, f):
    lowest = None               
    l = [l for l, u in intervals]
    u = [u for l, u in intervals]

    for s in range(N):
        r = np.random.uniform(l, u)
        print("iteration:", s, "trying out:", r)
        v = f(*r)
        if (not lowest) or lowest[0] > v:
            lowest = (v.copy(), r.copy())
    return lowest
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
v = global_random_search([(0.001, 0.1), (0.5,0.99), (0.5,0.99), (1, 128)], 20, f)
#+end_src

#+RESULTS:
#+begin_example
iteration: 0 trying out: [0.09498289 0.60721598 0.96816385 5.77365785]
epoch:  1, train_loss: 2.4407, train_accuracy: 10.09, test_loss: 2.3244, test_accuracy: 9.74
iteration: 1 trying out: [ 0.01967676  0.70659543  0.94288784 16.91781857]
epoch:  1, train_loss: 0.2771, train_accuracy: 92.45, test_loss: 0.2462, test_accuracy: 93.29
iteration: 2 trying out: [ 0.06621171  0.72099497  0.60460773 53.51220874]
epoch:  1, train_loss: 2.6385, train_accuracy: 10.48, test_loss: 2.3087, test_accuracy: 10.09
iteration: 3 trying out: [3.00407268e-02 5.67995762e-01 8.97930568e-01 8.84358207e+01]
epoch:  1, train_loss: 2.3674, train_accuracy: 10.95, test_loss: 2.3025, test_accuracy: 10.09
iteration: 4 trying out: [ 0.03034571  0.69081222  0.54135383 27.30341413]
epoch:  1, train_loss: 0.4284, train_accuracy: 90.77, test_loss: 0.4298, test_accuracy: 92.94
iteration: 5 trying out: [9.54651594e-03 9.05080222e-01 7.26347607e-01 3.80119077e+01]
epoch:  1, train_loss: 0.1914, train_accuracy: 94.54, test_loss: 0.1038, test_accuracy: 96.80
iteration: 6 trying out: [1.50627657e-02 5.41365557e-01 5.72977850e-01 8.94787475e+01]
epoch:  1, train_loss: 0.1848, train_accuracy: 94.77, test_loss: 0.0994, test_accuracy: 97.05
iteration: 7 trying out: [1.49200146e-03 8.97974129e-01 9.57523196e-01 9.76863517e+01]
epoch:  1, train_loss: 0.1396, train_accuracy: 95.71, test_loss: 0.0460, test_accuracy: 98.40
iteration: 8 trying out: [4.88600576e-02 5.78893000e-01 6.66134480e-01 5.37994719e+01]
epoch:  1, train_loss: 2.4444, train_accuracy: 10.56, test_loss: 2.3070, test_accuracy: 10.09
iteration: 9 trying out: [3.22257446e-02 5.34966512e-01 8.49675051e-01 3.43241725e+01]
epoch:  1, train_loss: 2.3417, train_accuracy: 10.59, test_loss: 2.3054, test_accuracy: 10.09
iteration: 10 trying out: [ 0.08814422  0.66662314  0.68038366 69.40305009]
epoch:  1, train_loss: 3.4640, train_accuracy: 10.60, test_loss: 2.3076, test_accuracy: 10.09
iteration: 11 trying out: [9.67306009e-03 8.94031649e-01 9.62538336e-01 4.23363962e+01]
epoch:  1, train_loss: 0.1310, train_accuracy: 95.98, test_loss: 0.0526, test_accuracy: 98.24
iteration: 12 trying out: [1.10849748e-03 8.85089227e-01 9.02617084e-01 1.14484117e+02]
epoch:  1, train_loss: 0.1593, train_accuracy: 95.19, test_loss: 0.0552, test_accuracy: 98.06
iteration: 13 trying out: [3.11733020e-02 9.38790910e-01 5.43791971e-01 6.86113916e+01]
epoch:  1, train_loss: 2.9801, train_accuracy: 10.72, test_loss: 2.3050, test_accuracy: 10.09
iteration: 14 trying out: [4.78196691e-02 7.92862800e-01 7.36652633e-01 6.63318745e+01]
epoch:  1, train_loss: 0.5027, train_accuracy: 89.28, test_loss: 0.2637, test_accuracy: 92.58
iteration: 15 trying out: [ 0.09514511  0.5803702   0.63155081 26.10744429]
epoch:  1, train_loss: 2.5394, train_accuracy: 10.49, test_loss: 2.3240, test_accuracy: 10.10
iteration: 16 trying out: [6.44511105e-02 7.06136749e-01 8.91964359e-01 1.27586204e+02]
epoch:  1, train_loss: 0.8502, train_accuracy: 89.59, test_loss: 0.2154, test_accuracy: 94.04
iteration: 17 trying out: [3.00972902e-02 6.71710049e-01 9.54459350e-01 1.00861255e+02]
epoch:  1, train_loss: 2.3845, train_accuracy: 10.92, test_loss: 2.3022, test_accuracy: 10.09
iteration: 18 trying out: [0.06069404 0.89793332 0.56147585 6.14011418]
epoch:  1, train_loss: 10433.2803, train_accuracy: 10.37, test_loss: 2.3389, test_accuracy: 9.74
iteration: 19 trying out: [4.18852755e-02 9.14906673e-01 8.48406021e-01 1.04010526e+02]
epoch:  1, train_loss: 2.4894, train_accuracy: 10.65, test_loss: 2.3032, test_accuracy: 10.28
#+end_example

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
v
#+end_src

#+RESULTS:
| array | (0.04600779 dtype=float32) | array | ((0.00149200146 0.897974129 0.957523196 97.6863517)) |

(0.04600779 dtype=float32) ((0.00149200146 0.897974129 0.957523196 97.6863517))
global_random_search([(0.001, 0.1), (0.5,0.99), (0.5,0.99), (1, 128)], 20, f)
function evaluated by error on test values

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
  learning_rate = 0.0015
  beta1 = 0.898
  beta2 = 0.9575
  batch_size = 98
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
v2 = global_random_search([(0.4, 0.8), (40, 90)], 20, f2)
#+end_src

#+RESULTS:
#+begin_example
iteration: 0 trying out: [ 0.55953832 65.88764716]
epoch:  1, train_loss: 0.1795, train_accuracy: 94.33, test_loss: 0.0689, test_accuracy: 97.73
iteration: 1 trying out: [ 0.40742066 78.18829847]
epoch:  1, train_loss: 0.2034, train_accuracy: 93.78, test_loss: 0.0629, test_accuracy: 97.93
iteration: 2 trying out: [ 0.48917857 48.61637121]
epoch:  1, train_loss: 0.1881, train_accuracy: 94.15, test_loss: 0.0463, test_accuracy: 98.48
iteration: 3 trying out: [ 0.57043057 50.54181151]
epoch:  1, train_loss: 0.1955, train_accuracy: 93.90, test_loss: 0.0485, test_accuracy: 98.40
iteration: 4 trying out: [ 0.61076742 81.02891856]
epoch:  1, train_loss: 0.2556, train_accuracy: 92.18, test_loss: 0.0608, test_accuracy: 98.00
iteration: 5 trying out: [ 0.57903954 59.75461617]
epoch:  1, train_loss: 0.1844, train_accuracy: 94.30, test_loss: 0.0625, test_accuracy: 97.87
iteration: 6 trying out: [ 0.49462337 63.32300952]
epoch:  1, train_loss: 0.1787, train_accuracy: 94.37, test_loss: 0.0553, test_accuracy: 98.15
iteration: 7 trying out: [ 0.49765613 48.23733849]
epoch:  1, train_loss: 0.1796, train_accuracy: 94.44, test_loss: 0.0676, test_accuracy: 97.76
iteration: 8 trying out: [ 0.43409191 83.42276451]
epoch:  1, train_loss: 0.2032, train_accuracy: 93.76, test_loss: 0.0582, test_accuracy: 98.05
iteration: 9 trying out: [ 0.66329857 55.58680994]
epoch:  1, train_loss: 0.1800, train_accuracy: 94.26, test_loss: 0.0466, test_accuracy: 98.37
iteration: 10 trying out: [ 0.43736516 40.5431808 ]
epoch:  1, train_loss: 0.1445, train_accuracy: 95.39, test_loss: 0.0496, test_accuracy: 98.29
iteration: 11 trying out: [ 0.52908237 40.42440148]
epoch:  1, train_loss: 0.1559, train_accuracy: 95.12, test_loss: 0.0690, test_accuracy: 97.67
iteration: 12 trying out: [ 0.56054044 86.27810951]
epoch:  1, train_loss: 0.2092, train_accuracy: 93.45, test_loss: 0.0675, test_accuracy: 97.71
iteration: 13 trying out: [ 0.51109967 69.7771212 ]
epoch:  1, train_loss: 0.1857, train_accuracy: 94.10, test_loss: 0.0566, test_accuracy: 98.05
iteration: 14 trying out: [ 0.62437753 80.04489905]
epoch:  1, train_loss: 0.3337, train_accuracy: 89.28, test_loss: 0.0775, test_accuracy: 97.48
iteration: 15 trying out: [ 0.7589336  51.38229871]
epoch:  1, train_loss: 0.2628, train_accuracy: 91.58, test_loss: 0.0620, test_accuracy: 97.90
iteration: 16 trying out: [ 0.42258326 51.52417999]
epoch:  1, train_loss: 0.1600, train_accuracy: 94.98, test_loss: 0.0528, test_accuracy: 98.08
iteration: 17 trying out: [ 0.69335974 84.99384614]
epoch:  1, train_loss: 0.2936, train_accuracy: 90.70, test_loss: 0.0634, test_accuracy: 97.89
iteration: 18 trying out: [ 0.63986219 62.57943355]
epoch:  1, train_loss: 0.1975, train_accuracy: 93.74, test_loss: 0.0468, test_accuracy: 98.55
iteration: 19 trying out: [ 0.56126106 49.44887083]
epoch:  1, train_loss: 0.1682, train_accuracy: 94.66, test_loss: 0.0485, test_accuracy: 98.46
#+end_example

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
print(v2)
#+end_src

#+RESULTS:
: (array(0.04633828, dtype=float32), array([ 0.48917857, 48.61637121]))

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
# (array(0.05145077, dtype=float32), array([ 0.49315356, 58.39919518]))
 # (array(0.05257225, dtype=float32), array([ 0.75313327, 93.05358694]))
# (array(0.04633828, dtype=float32), array([ 0.48917857, 48.61637121]))
learning_rate = 0.489
batch_size = 49
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
# opt = optax.sgd(learning_rate=0.1)
opt = optax.adam(learning_rate=0.001, b1=0.9, b2=0.999)
cfg = get_config(opt=opt, batch_size=128)
# state, loss_history, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
print(len(loss_history))
print(len(train_ds['label'])/128)
#+end_src

#+RESULTS:
: 468
: 468.75

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
plt.plot(range(len(loss_history)), loss_history)
#+end_src
*** Loss Histories

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def sgdf(learning_rate, batch_size):
    opt = optax.sgd(learning_rate=learning_rate)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, loss_history, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
    return loss_history, test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def adamf(learning_rate, b1, b2, batch_size):
    opt = optax.adam(learning_rate=learning_rate, b1=b1, b2=b2)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, loss_history, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
    return loss_history, test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def run_multiple(runs, f):
    # need to thread random seed
    
    loss_histories = []
    test_losses = []

    for r in range(runs):
        print("Run number:", r)
        loss_history, test_loss = f()
        loss_histories += [loss_history]
        test_losses += [test_loss]
    return loss_histories, test_losses
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
sgd_default_alpha = 0.1
sgd_default_batch = 128
sgd_default = lambda: sgdf(sgd_default_alpha, sgd_default_batch)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
sgd_optimal_alpha = 0.489
sgd_optimal_batch = 49
sgd_optimal = lambda: sgdf(sgd_optimal_alpha, sgd_optimal_batch)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
adam_default_alpha = 0.01
adam_default_b1 = 0.9
adam_default_b2 = 0.999
adam_default_batch = 128
adam_default = lambda: adamf(adam_default_alpha, adam_default_b1, adam_default_b2, adam_default_batch)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
adam_optimal_alpha = 0.0015
adam_optimal_b1 = 0.898
adam_optimal_b2 = 0.9575
adam_optimal_batch = 98
adam_optimal = lambda: adamf(adam_optimal_alpha, adam_optimal_b1, adam_optimal_b2, adam_optimal_batch)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
runs = 2
sgd_default_loss_histories, sgd_default_test_losses = run_multiple(runs, sgd_default)
#+end_src

#+RESULTS:
: Run number: 0
: epoch:  1, train_loss: 0.3880, train_accuracy: 88.29, test_loss: 0.1423, test_accuracy: 95.48
: Run number: 1
: epoch:  1, train_loss: 0.3880, train_accuracy: 88.29, test_loss: 0.1423, test_accuracy: 95.48

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
print(sgd_default_test_losses)
#+end_src

#+RESULTS:
: [DeviceArray(0.14234129, dtype=float32), DeviceArray(0.14234129, dtype=float32)]

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
runs = 2
print("SGD Default")
sgd_default_loss_histories, sgd_default_test_losses = run_multiple(runs, sgd_default)

print("SGD Optimal")
sgd_optimal_loss_histories, sgd_optimal_test_losses = run_multiple(runs, sgd_optimal)

print("Adam Default")
adam_default_loss_histories, adam_default_test_losses = run_multiple(runs, adam_default)

print("Adam Optimal")
adam_optimal_loss_histories, adam_optimal_test_losses = run_multiple(runs, adam_optimal)
#+end_src

** sst-2

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
"""Trains an SST2 text classifier."""
from typing import Any, Callable, Dict, Iterable, Optional, Sequence, Tuple, Union

from absl import logging
from flax import struct
from flax.metrics import tensorboard
from flax.training import train_state
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
import optax
import tensorflow as tf

import input_pipeline
import models


Array = jnp.ndarray
Example = Dict[str, Array]
TrainState = train_state.TrainState


class Metrics(struct.PyTreeNode):
  """Computed metrics."""
  loss: float
  accuracy: float
  count: Optional[int] = None


@jax.vmap
def sigmoid_cross_entropy_with_logits(*, labels: Array, logits: Array) -> Array:
  """Sigmoid cross entropy loss."""
  zeros = jnp.zeros_like(logits, dtype=logits.dtype)
  condition = (logits >= zeros)
  relu_logits = jnp.where(condition, logits, zeros)
  neg_abs_logits = jnp.where(condition, -logits, logits)
  return relu_logits - logits * labels + jnp.log1p(jnp.exp(neg_abs_logits))


def get_initial_params(rng, model):
  """Returns randomly initialized parameters."""
  token_ids = jnp.ones((2, 3), jnp.int32)
  lengths = jnp.ones((2,), dtype=jnp.int32)
  variables = model.init(rng, token_ids, lengths, deterministic=True)
  return variables['params']


def create_train_state(rng, config: ml_collections.ConfigDict, model):
  """Create initial training state."""
  params = get_initial_params(rng, model)
  tx = optax.chain(
      optax.sgd(learning_rate=config.learning_rate, momentum=config.momentum),
      optax.additive_weight_decay(weight_decay=config.weight_decay))
  state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)
  return state


def compute_metrics(*, labels: Array, logits: Array) -> Metrics:
  """Computes the metrics, summed across the batch if a batch is provided."""
  if labels.ndim == 1:  # Prevent the labels from broadcasting over the logits.
    labels = jnp.expand_dims(labels, axis=1)
  loss = sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)
  binary_predictions = (logits >= 0.)
  binary_accuracy = jnp.equal(binary_predictions, labels)
  return Metrics(
      loss=jnp.sum(loss),
      accuracy=jnp.sum(binary_accuracy),
      count=logits.shape[0])


def model_from_config(config: ml_collections.ConfigDict):
  """Builds a text classification model from a config."""
  model = models.TextClassifier(
      embedding_size=config.embedding_size,
      hidden_size=config.hidden_size,
      vocab_size=config.vocab_size,
      output_size=config.output_size,
      dropout_rate=config.dropout_rate,
      word_dropout_rate=config.word_dropout_rate,
      unk_idx=config.unk_idx)
  return model


def train_step(
    state: TrainState,
    batch: Dict[str, Array],
    rngs: Dict[str, Any],
) -> Tuple[TrainState, Metrics]:
  """Train for a single step."""
  # Make sure to get a new RNG at every step.
  step = state.step
  rngs = {name: jax.random.fold_in(rng, step) for name, rng in rngs.items()}

  def loss_fn(params):
    variables = {'params': params}
    logits = state.apply_fn(
        variables, batch['token_ids'], batch['length'],
        deterministic=False,
        rngs=rngs)

    labels = batch['label']
    if labels.ndim == 1:
      labels = jnp.expand_dims(labels, 1)
    loss = jnp.mean(
        sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))
    return loss, logits

  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
  value, grads = grad_fn(state.params)
  (_, logits) = value

  new_state = state.apply_gradients(grads=grads)
  metrics = compute_metrics(labels=batch['label'], logits=logits)
  return new_state, metrics


def eval_step(state: TrainState, batch: Dict[str, Array],
              rngs: Dict[str, Any]) -> Metrics:
  """Evaluate for a single step. Model should be in deterministic mode."""
  variables = {'params': state.params}
  logits = state.apply_fn(
      variables, batch['token_ids'], batch['length'],
      deterministic=True,
      rngs=rngs)
  metrics = compute_metrics(labels=batch['label'], logits=logits)
  return metrics


def normalize_batch_metrics(
        batch_metrics: Sequence[Metrics]) -> Metrics:
  """Consolidates and normalizes a list of per-batch metrics dicts."""
  # Here we sum the metrics that were already summed per batch.
  total_loss = np.sum([metrics.loss for metrics in batch_metrics])
  total_accuracy = np.sum([metrics.accuracy for metrics in batch_metrics])
  total = np.sum([metrics.count for metrics in batch_metrics])
  # Divide each metric by the total number of items in the data set.
  return Metrics(
      loss=total_loss.item() / total, accuracy=total_accuracy.item() / total)


def batch_to_numpy(batch: Dict[str, tf.Tensor]) -> Dict[str, Array]:
  """Converts a batch with TF tensors to a batch of NumPy arrays."""
  # _numpy() reuses memory, does not make a copy.
  # pylint: disable=protected-access
  return jax.tree_map(lambda x: x._numpy(), batch)


def evaluate_model(
        eval_step_fn: Callable[..., Any],
        state: TrainState,
        batches: Union[Iterable[Example], tf.data.Dataset],
        epoch: int,
        rngs: Optional[Dict[str, Any]] = None
) -> Metrics:
  """Evaluate a model on a dataset."""
  batch_metrics = []
  for i, batch in enumerate(batches):
    batch = batch_to_numpy(batch)
    if rngs is not None:  # New RNG for each step.
      rngs = {name: jax.random.fold_in(rng, i) for name, rng in rngs.items()}

    metrics = eval_step_fn(state, batch, rngs)
    batch_metrics.append(metrics)

  batch_metrics = jax.device_get(batch_metrics)
  metrics = normalize_batch_metrics(batch_metrics)
  logging.info('eval  epoch %03d loss %.4f accuracy %.2f', epoch,
               metrics.loss, metrics.accuracy * 100)
  return metrics


def train_epoch(train_step_fn: Callable[..., Tuple[TrainState, Metrics]],
                state: TrainState,
                train_batches: tf.data.Dataset,
                epoch: int,
                rngs: Optional[Dict[str, Any]] = None
                ) -> Tuple[TrainState, Metrics]:
  """Train for a single epoch."""
  batch_metrics = []
  for batch in train_batches:
    batch = batch_to_numpy(batch)
    state, metrics = train_step_fn(state, batch, rngs)
    batch_metrics.append(metrics)

  # Compute the metrics for this epoch.
  batch_metrics = jax.device_get(batch_metrics)
  metrics = normalize_batch_metrics(batch_metrics)

  logging.info('train epoch %03d loss %.4f accuracy %.2f', epoch,
               metrics.loss, metrics.accuracy * 100)

  return state, metrics


def train_and_evaluate(config: ml_collections.ConfigDict,
                       workdir: str) -> TrainState:
  """Execute model training and evaluation loop.
  Args:
    config: Hyperparameter configuration for training and evaluation.
    workdir: Directory where the tensorboard summaries are written to.
  Returns:
    The final train state that includes the trained parameters.
  """
  # Prepare datasets.
  train_dataset = input_pipeline.TextDataset(
      tfds_name='glue/sst2', split='train')
  eval_dataset = input_pipeline.TextDataset(
      tfds_name='glue/sst2', split='validation')
  train_batches = train_dataset.get_bucketed_batches(
      config.batch_size,
      config.bucket_size,
      max_input_length=config.max_input_length,
      drop_remainder=True,
      shuffle=True,
      shuffle_seed=config.seed)
  eval_batches = eval_dataset.get_batches(batch_size=config.batch_size)

  # Keep track of vocab size in the config so that the embedder knows it.
  config.vocab_size = len(train_dataset.vocab)

  # Compile step functions.
  train_step_fn = jax.jit(train_step)
  eval_step_fn = jax.jit(eval_step)

  # Create model and a state that contains the parameters.
  rng = jax.random.PRNGKey(config.seed)
  model = model_from_config(config)
  state = create_train_state(rng, config, model)

  summary_writer = tensorboard.SummaryWriter(workdir)
  summary_writer.hparams(dict(config))

  # Main training loop.
  logging.info('Starting training...')
  for epoch in range(1, config.num_epochs + 1):

    # Train for one epoch.
    rng, epoch_rng = jax.random.split(rng)
    rngs = {'dropout': epoch_rng}
    state, train_metrics = train_epoch(
        train_step_fn, state, train_batches, epoch, rngs)

    # Evaluate current model on the validation data.
    eval_metrics = evaluate_model(eval_step_fn, state, eval_batches, epoch)

    # Write metrics to TensorBoard.
    summary_writer.scalar('train_loss', train_metrics.loss, epoch)
    summary_writer.scalar(
        'train_accuracy',
        train_metrics.accuracy * 100,
        epoch)
    summary_writer.scalar('eval_loss', eval_metrics.loss, epoch)
    summary_writer.scalar(
        'eval_accuracy',
        eval_metrics.accuracy * 100,
        epoch)

  summary_writer.flush()
  return state
#+end_src

* Appendix
** Code Listing
#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\lstinputlisting[language=Python]{FinalSrc.py}
#+end_export

