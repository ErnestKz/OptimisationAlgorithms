#+AUTHOR:Ernests Kuznecovs - 17332791 - kuznecoe@tcd.ie
#+Date:2021-2022
#+Title:Optimisation Algorithms - Final Assignment

#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
    }
\lstset{style=mystyle}
#+end_export

* Preamble                                                         :noexport:
#+PROPERTY: header-args:python :session fa
#+PROPERTY: header-args:python+ :async yes
#+PROPERTY: header-args:python+ :eval never-export
#+PROPERTY: header-args:elisp :eval never-export
#+EXCLUDE_TAGS: noexport
#+STARTUP: overview
#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage[a4paper, total={6.7in, 10.5in}]{geometry}

#+LaTeX_HEADER: \usepackage{caption}
#+LaTeX_HEADER: \newcommand\figwidth{0.48}

#+begin_src elisp :results none :exports none
(setq-local org-image-actual-width '(512))
(setq-local org-confirm-babel-evaluate nil)
(setq-local org-src-preserve-indentation 't)

(setq org-latex-listings t)
(setq org-latex-prefer-user-labels t)
#+end_src

#+begin_src elisp :results none :exports none
(use-package jupyter
  :config
  (org-babel-do-load-languages 'org-babel-load-languages '((emacs-lisp . t)
							   (python . t)
							   (jupyter . t)))
  (org-babel-jupyter-override-src-block "python")
  (add-hook 'org-babel-after-execute-hook 'org-redisplay-inline-images)
  (org-babel-do-load-languages
   'org-babel-load-languages
   '((emacs-lisp . t)
     (python . t)
     (jupyter . t))))
#+end_src

* Python Imports                                                   :noexport:

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 200
mpl.rcParams['figure.facecolor'] = '1'
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')

import copy
import numpy as np
from sklearn import metrics
#+end_src

* Flax, Jax, Optax Examples                                        :noexport:

** Flax
#+begin_src python :results none :exports none :tangle ./FinalSrc.py
from typing import Sequence

import numpy as np
import jax
import jax.numpy as jnp
import flax.linen as nn

class MLP(nn.Module):
  features: Sequence[int]

  @nn.compact
  def __call__(self, x):
    for feat in self.features[:-1]:
      x = nn.relu(nn.Dense(feat)(x))
    x = nn.Dense(self.features[-1])(x)
    return x

model = MLP([12, 8, 4])
batch = jnp.ones((32, 10))
variables = model.init(jax.random.PRNGKey(0), batch)
output = model.apply(variables, batch)
#+end_src

** Optax

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
import random
from typing import Tuple

import optax
import jax.numpy as jnp
import jax
import numpy as np

BATCH_SIZE = 5
NUM_TRAIN_STEPS = 1_000
RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))

TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)
LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
initial_params = {
    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),
    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),
}


def net(x: jnp.ndarray, params: jnp.ndarray) -> jnp.ndarray:
  x = jnp.dot(x, params['hidden'])
  x = jax.nn.relu(x)
  x = jnp.dot(x, params['output'])
  return x


def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:
  y_hat = net(batch, params)

  # optax also provides a number of common loss functions.
  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)

  return loss_value.mean()
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:
  opt_state = optimizer.init(params)

  @jax.jit
  def step(params, opt_state, batch, labels):
    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)
    updates, opt_state = optimizer.update(grads, opt_state, params)
    params = optax.apply_updates(params, updates)
    return params, opt_state, loss_value

  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):
    params, opt_state, loss_value = step(params, opt_state, batch, labels)
    if i % 100 == 0:
      print(f'step {i}, loss: {loss_value}')

  return params

# Finally, we can fit our parametrized function using the Adam optimizer
# provided by optax.
optimizer = optax.adam(learning_rate=1e-2)
optimizer2 = optax.sgd(learning_rate=1e-2)
params = fit(initial_params, optimizer)
params = fit(initial_params, optimizer2)
#+end_src

* Assignment                                                       :noexport:

- Need to complete declaration.
- Include code as text.
- Porgrams should be running code.
- Reports should be 5 pages, 10 pages upper limit


- Comparing performance of SGD with
  - Adam
  - Constant Step size

    
- To do this need to make important choices.

  
  - How to measure performance.
    - e.g plot ML loss function vs optimisation iterations
      - use lowest value as performance measure
	- but this measures performance on training data, not on unseen (non-generalised)
    - e.g measure ML loss function on held-out test data
    - good idea to look at both measures

  - SGD involves randomisation
    - may be necessary to collect data from several runs
      - to understand how performance fluctuates from run to run

  - What hyperparameters to use and how to choose them.
    - Look at performance of both when using
      - default hyperparameter values
      - and when using optimised values (global random search?)

  - What ML model and data to use for evaluation.
    - probably worth 2 models/datasets
    - at least 1 neural net ML model
    - MNIST, CIFAR, Imbd

  - Existing examples of performance evaluation
    - Adam: A Method For Stochastic Optimization
      - https://arxiv.org/pdf/1412.6980.pdf
      - Training error vs other algorithms
    - The Marginal Value of Adaptive Gradient Methods in Machine Learning
      - https://arxiv.org/pdf/1705.08292.pdf
      - Test error (i.e generalisaton)
      - of SGD against a range of algorithms, including Adam

    - Might reflect on, do these papers address choices noted above?
      - if not, might it be important or not?

* Libraries, Documentation, Resources                              :noexport:
** Optax - Optimisation Algorithms Library for Jax
- https://optax.readthedocs.io/en/latest/api.html#sgd
- https://optax.readthedocs.io/en/latest/api.html#adam
- https://optax.readthedocs.io/en/latest/

** Flax - Neural Network Library for Jax
- AiEpiphany
  - https://www.youtube.com/watch?v=5eUSmJvK8WA&t=13s
  - https://github.com/gordicaleksa/get-started-with-JAX/blob/main/Tutorial_4_Flax_Zero2Hero_Colab.ipynb
    
- https://github.com/google/flax
  - Can use MNIST, CIFAR10 example
  
** Jax
- https://colinraffel.com/blog/you-don-t-know-jax.html
- AiEpiphany
  - Part 1 - https://www.youtube.com/watch?v=SstuvS-tVc0&t=1649s
  - Part 2 - https://www.youtube.com/watch?v=CQQaifxuFcs&t=62s
  - Part 3 - https://www.youtube.com/watch?v=6_PqUPxRmjY&t=1155s

* Tasks                                                            :noexport:
- Find one more model (text analytics one?)

- Can start writing prose for MNIST
  - params
  - make plots look nice
  - talk about plots

* Notes on Flax, Jax, Optax                                        :noexport:
- SGD implemented by chaining in Optax
  
* Tests and Evaluation                                             :noexport:
** Characteristics of the Domain
- Constant vs Adam

- Both have randomness.  
- Both have batch size.

- Probably constant, common epochs.

- Constant:
  - Alpha 
- Adam:
  - Alpha
  - Beta1
  - Beta2

- Default params for Adam:
  - Alpha = 0.001
  - Beta1 = 0.9
  - Beta2 = 0.999
- Take 0.01 as default for Constant.


- Then picking good hyperparameter values.
  - Global Random Search
    
** Evaluations and Visualisation

- Can't really have contour plot without quite a bit of effort.
  - Would have to look at 2 parameters at a time.
    - Perhaps see how countour of 2 parameters change over time, as other parameters are changed.

- With default hyperparams:

  - Plot:
    - Loss Function vs Optimisation Iteration (With error bars perhaps)

  - Boxplot:
    - Lowest value of loss function as performance measure. (non-generalised)
    - ML performance on held out data as performance measure. (generalised)

- With optimised hyperparams (Global Random Search):
  
  - Same stuff.

- Data for Plot ad Boxplot can be gethered in the same runs.

* Datasets and Models                                              :noexport:
- https://github.com/google/flax/tree/main/examples/lm1b
- https://github.com/google/flax/tree/main/examples/mnist
- https://github.com/google/flax/tree/main/examples/sst2

** mnist

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
from absl import logging
from flax import linen as nn
from flax.metrics import tensorboard
from flax.training import train_state
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
import optax
import tensorflow_datasets as tfds
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
class CNN(nn.Module):
  """A simple CNN model."""

  @nn.compact
  def __call__(self, x):
    x = nn.Conv(features=32, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = nn.Conv(features=64, kernel_size=(3, 3))(x)
    x = nn.relu(x)
    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))
    x = x.reshape((x.shape[0], -1))  # flatten
    x = nn.Dense(features=256)(x)
    x = nn.relu(x)
    x = nn.Dense(features=10)(x)
    return x
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
@jax.jit
def apply_model(state, images, labels):
  """Computes gradients, loss and accuracy for a single batch."""
  def loss_fn(params):
    logits = CNN().apply({'params': params}, images)
    one_hot = jax.nn.one_hot(labels, 10)
    loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))
    return loss, logits

  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
  (loss, logits), grads = grad_fn(state.params)
  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)
  return grads, loss, accuracy

@jax.jit
def update_model(state, grads):
  return state.apply_gradients(grads=grads)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def train_epoch(state, train_ds, batch_size, rng, loss_history):
  """Train for a single epoch."""
  train_ds_size = len(train_ds['image'])
  steps_per_epoch = train_ds_size // batch_size

  perms = jax.random.permutation(rng, len(train_ds['image']))
  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch
  perms = perms.reshape((steps_per_epoch, batch_size))

  epoch_loss = []
  epoch_accuracy = []

  for perm in perms:
    batch_images = train_ds['image'][perm, ...]
    batch_labels = train_ds['label'][perm, ...]
    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)
    state = update_model(state, grads)
    epoch_loss.append(loss)
    loss_history.append(loss)
    epoch_accuracy.append(accuracy)

  train_loss = np.mean(epoch_loss)
  train_accuracy = np.mean(epoch_accuracy)
  return state, train_loss, train_accuracy
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def get_datasets():
  """Load MNIST train and test datasets into memory."""
  ds_builder = tfds.builder('mnist')
  ds_builder.download_and_prepare()
  train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))
  test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))
  train_ds['image'] = jnp.float32(train_ds['image']) / 255.
  test_ds['image'] = jnp.float32(test_ds['image']) / 255.
  return train_ds, test_ds
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def create_train_state(rng, config):
  """Creates initial `TrainState`."""
  cnn = CNN()
  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']

  tx = config.optimiser
  
  return train_state.TrainState.create(
      apply_fn=cnn.apply, params=params, tx=tx)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def train_and_evaluate(config: ml_collections.ConfigDict,
                       workdir: str) -> train_state.TrainState:
  """Execute model training and evaluation loop.
  Args:
    config: Hyperparameter configuration for training and evaluation.
    workdir: Directory where the tensorboard summaries are written to.
  Returns:
    The train state (which includes the `.params`).
  """
  train_ds, test_ds = get_datasets()
  rng = jax.random.PRNGKey(0)

  summary_writer = tensorboard.SummaryWriter(workdir)
  summary_writer.hparams(dict(config))

  rng, init_rng = jax.random.split(rng)
  state = create_train_state(init_rng, config)

  for epoch in range(1, config.num_epochs + 1):
    rng, input_rng = jax.random.split(rng)
    state, train_loss, train_accuracy = train_epoch(state, train_ds,
                                                    config.batch_size,
                                                    input_rng)
    _, test_loss, test_accuracy = apply_model(state, test_ds['image'],
                                              test_ds['label'])

    logging.info(
        'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'
        % (epoch, train_loss, train_accuracy * 100, test_loss,
           test_accuracy * 100))

    summary_writer.scalar('train_loss', train_loss, epoch)
    summary_writer.scalar('train_accuracy', train_accuracy, epoch)
    summary_writer.scalar('test_loss', test_loss, epoch)
    summary_writer.scalar('test_accuracy', test_accuracy, epoch)
  summary_writer.flush()
  return state
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def train_and_evaluate(config: ml_collections.ConfigDict,
                       workdir: str,
                       train_ds,
                       test_ds,
                       seed):

  rng, init_rng = jax.random.split(seed)
  state = create_train_state(init_rng, config)
  
  _, test_loss, test_accuracy = apply_model(state, test_ds['image'], test_ds['label'])
  # print('epoch:% 3d, test_loss: %.4f, test_accuracy: %.2f'
  #         % (0, test_loss, test_accuracy * 100))


  loss_history = []
  
  for epoch in range(1, config.num_epochs + 1):
    rng, input_rng = jax.random.split(rng)
    state, train_loss, train_accuracy = train_epoch(state, train_ds, config.batch_size, input_rng, loss_history)
    _, test_loss, test_accuracy = apply_model(state, test_ds['image'], test_ds['label'])

    print('epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, test_loss: %.4f, test_accuracy: %.2f'
          % (epoch, train_loss, train_accuracy * 100, test_loss, test_accuracy * 100))
  return state, loss_history, test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def get_config(opt, batch_size):
  """Get the default hyperparameter configuration."""
  config = ml_collections.ConfigDict()
  config.optimiser = opt
  config.batch_size = batch_size
  config.num_epochs = 1
  return config
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
train_ds, test_ds = get_datasets()
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def f(learning_rate, b1, b2, batch_size):
    opt = optax.adam(learning_rate=learning_rate, b1=b1, b2=b2)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, _, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
    return test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def f2(learning_rate, batch_size):
    opt = optax.sgd(learning_rate=learning_rate)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, _, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
    return test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def global_random_search(intervals, N, f):
    lowest = None               
    l = [l for l, u in intervals]
    u = [u for l, u in intervals]

    for s in range(N):
        r = np.random.uniform(l, u)
        print("iteration:", s, "trying out:", r)
        v = f(*r)
        if (not lowest) or lowest[0] > v:
            lowest = (v.copy(), r.copy())
    return lowest
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
v = global_random_search([(0.001, 0.1), (0.5,0.99), (0.5,0.99), (1, 128)], 20, f)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
v
#+end_src

(0.04600779 dtype=float32) ((0.00149200146 0.897974129 0.957523196 97.6863517))
global_random_search([(0.001, 0.1), (0.5,0.99), (0.5,0.99), (1, 128)], 20, f)
function evaluated by error on test values

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
  learning_rate = 0.0015
  beta1 = 0.898
  beta2 = 0.9575
  batch_size = 98
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
v2 = global_random_search([(0.4, 0.8), (40, 90)], 20, f2)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
print(v2)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
# (array(0.05145077, dtype=float32), array([ 0.49315356, 58.39919518]))
 # (array(0.05257225, dtype=float32), array([ 0.75313327, 93.05358694]))
# (array(0.04633828, dtype=float32), array([ 0.48917857, 48.61637121]))
learning_rate = 0.489
batch_size = 49
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
# opt = optax.sgd(learning_rate=0.1)
opt = optax.adam(learning_rate=0.001, b1=0.9, b2=0.999)
cfg = get_config(opt=opt, batch_size=128)
# state, loss_history, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
print(len(loss_history))
print(len(train_ds['label'])/128)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
plt.plot(range(len(loss_history)), loss_history)
#+end_src

*** Loss Histories

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def sgdf(learning_rate, batch_size, seed):
    opt = optax.sgd(learning_rate=learning_rate)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, loss_history, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds, seed)
    return loss_history, test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def adamf(learning_rate, b1, b2, batch_size, seed):
    opt = optax.adam(learning_rate=learning_rate, b1=b1, b2=b2)
    cfg = get_config(opt=opt, batch_size=round(batch_size))
    _, loss_history, test_loss = train_and_evaluate(cfg, "./mnist/", train_ds, test_ds, seed)
    return loss_history, test_loss
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def run_multiple(runs, f):
    # need to thread random seed
    
    loss_histories = []
    test_losses = []

    seed = jax.random.PRNGKey(0)
    seed, subseed = jax.random.split(seed)
    
    for r in range(runs):
        print("Run number:", r)
        loss_history, test_loss = f(subseed)
        seed, subseed = jax.random.split(seed)
        loss_histories += [loss_history]
        test_losses += [test_loss]
    return loss_histories, test_losses
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
sgd_default_alpha = 0.1
sgd_default_batch = 128
sgd_default = lambda seed: sgdf(sgd_default_alpha, sgd_default_batch,seed=seed)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
sgd_optimal_alpha = 0.489
sgd_optimal_batch = 49
sgd_optimal = lambda seed: sgdf(sgd_optimal_alpha, sgd_optimal_batch, seed=seed)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
adam_default_alpha = 0.01
adam_default_b1 = 0.9
adam_default_b2 = 0.999
adam_default_batch = 128
adam_default = lambda seed: adamf(adam_default_alpha, adam_default_b1, adam_default_b2, adam_default_batch, seed=seed)
#+end_src

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
adam_optimal_alpha = 0.0015
adam_optimal_b1 = 0.898
adam_optimal_b2 = 0.9575
adam_optimal_batch = 98
adam_optimal = lambda seed: adamf(adam_optimal_alpha, adam_optimal_b1, adam_optimal_b2, adam_optimal_batch, seed=seed)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
runs = 2
sgd_default_loss_histories, sgd_default_test_losses = run_multiple(runs, sgd_default)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
print(sgd_default_test_losses)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
runs = 20
print("SGD Default")
sgd_default_loss_histories, sgd_default_test_losses = run_multiple(runs, sgd_default)

print("SGD Optimal")
sgd_optimal_loss_histories, sgd_optimal_test_losses = run_multiple(runs, sgd_optimal)

print("Adam Default")
adam_default_loss_histories, adam_default_test_losses = run_multiple(runs, adam_default)

print("Adam Optimal")
adam_optimal_loss_histories, adam_optimal_test_losses = run_multiple(runs, adam_optimal)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
import pickle

mlruns = {
    "sgd_default_loss_histories": sgd_default_loss_histories,
    "sgd_default_test_losses": sgd_default_test_losses,
    "sgd_optimal_loss_histories": sgd_optimal_loss_histories,
    "sgd_optimal_test_losses": sgd_optimal_test_losses,
    
    "adam_default_loss_histories": adam_default_loss_histories,
    "adam_default_test_losses": adam_default_test_losses,
    "adam_optimal_loss_histories": adam_optimal_loss_histories,
    "adam_optimal_test_losses": adam_optimal_test_losses
}

pickle.dump(mlruns, open("mlruns.p", "wb"))
#+end_src

*** Loading and Plotting

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
import pickle
mlruns_l = pickle.load(open( "mlruns.p", "rb" ))
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
mlruns_l.keys()
#+end_src

#+RESULTS:
: dict_keys(['sgd_default_loss_histories', 'sgd_default_test_losses', 'sgd_optimal_loss_histories', 'sgd_optimal_test_losses', 'adam_default_loss_histories', 'adam_default_test_losses', 'adam_optimal_loss_histories', 'adam_optimal_test_losses'])

Will need min and max of each iteration.

plot(iter, average_on_iter_i)
fill_between(iter, min_on_iter_i, max_on_iter_i)

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def plot_history(losses):
    'losses :: [[float]], ith element is loss vs iteration of ith run of the SGD'
    losses = np.array(losses)
    average_on_iter_i = np.mean(losses, axis=0)
    min_on_iter_i = np.minimum.reduce(losses)
    max_on_iter_i = np.maximum.reduce(losses)
    x = range(len(average_on_iter_i))
    plt.plot(x, average_on_iter_i , 'k-')
    plt.fill_between(x, min_on_iter_i, max_on_iter_i)

def avg_max_min(loss_histories):
    average_on_iter_i = np.mean(loss_histories, axis=0)
    min_on_iter_i = np.minimum.reduce(loss_histories)
    max_on_iter_i = np.maximum.reduce(loss_histories)
    return average_on_iter_i, min_on_iter_i, max_on_iter_i
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
plot_history(mlruns_l['sgd_default_loss_histories'])
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
plot_history(mlruns_l['sgd_optimal_loss_histories'])
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
plot_history(mlruns_l['adam_default_loss_histories'])
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
plot_history(mlruns_l['adam_optimal_loss_histories'])
#+end_src


- Fix the axis
- Scale the x axis to datapoints used.  

- Is larger batch size faster to get through the epoch?





the x will be real numbers then
np.linspace(start, stop, num)

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
np.array(mlruns_l['sgd_default_loss_histories']).shape
#+end_src

#+RESULTS:
| 20 | 468 |

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
np.array(mlruns_l['sgd_optimal_loss_histories']).shape
#+end_src

#+RESULTS:
| 20 | 1224 |

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
def compare_sgd(r1, r2):
    r1 = np.array(r1) ; r2 = np.array(r2)
    xr = r1.shape[1]  ; xr2 = r2.shape[1]
    x1 = np.linspace(0, 100, xr)
    x2 = np.linspace(0, 100, xr2)
    a1, l1, h1 = avg_max_min(r1)
    a2, l2, h2 = avg_max_min(r2)

    plt.semilogy(x1, a1 , 'b-')
    plt.fill_between(x1, l1, h1)
    xlim = plt.xlim()
    ylim = plt.ylim()
    plt.semilogy(x2, a2 , 'k-')
    plt.fill_between(x2, l2, h2)
    plt.xlim(xlim)
    plt.ylim(ylim)

    plt.xlabel(r'%epoch')
    plt.ylabel(r'loss')
    # plt.title("default vs optimal")
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
r1 = np.array(mlruns_l['sgd_default_loss_histories'])
r2 = np.array(mlruns_l['sgd_optimal_loss_histories'])

compare_sgd(r1, r2)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
r1 = np.array(mlruns_l['adam_default_loss_histories'])
r2 = np.array(mlruns_l['adam_optimal_loss_histories'])
compare_sgd(r1, r2)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
r1 = np.array(mlruns_l['sgd_optimal_loss_histories'])
r2 = np.array(mlruns_l['adam_optimal_loss_histories'])
compare_sgd(r1, r2)
#+end_src

#+begin_src python :results replace :exports none :tangle ./FinalSrc.py
r1 = np.array(mlruns_l['sgd_default_loss_histories'])
r2 = np.array(mlruns_l['adam_default_loss_histories'])
compare_sgd(r1, r2)
#+end_src

Optimal is scaled down to the scale of default.
Have more datapoints for optimal per x

** sst-2

#+begin_src python :results none :exports none :tangle ./FinalSrc.py
"""Trains an SST2 text classifier."""
from typing import Any, Callable, Dict, Iterable, Optional, Sequence, Tuple, Union

from absl import logging
from flax import struct
from flax.metrics import tensorboard
from flax.training import train_state
import jax
import jax.numpy as jnp
import ml_collections
import numpy as np
import optax
import tensorflow as tf

import input_pipeline
import models


Array = jnp.ndarray
Example = Dict[str, Array]
TrainState = train_state.TrainState


class Metrics(struct.PyTreeNode):
  """Computed metrics."""
  loss: float
  accuracy: float
  count: Optional[int] = None


@jax.vmap
def sigmoid_cross_entropy_with_logits(*, labels: Array, logits: Array) -> Array:
  """Sigmoid cross entropy loss."""
  zeros = jnp.zeros_like(logits, dtype=logits.dtype)
  condition = (logits >= zeros)
  relu_logits = jnp.where(condition, logits, zeros)
  neg_abs_logits = jnp.where(condition, -logits, logits)
  return relu_logits - logits * labels + jnp.log1p(jnp.exp(neg_abs_logits))


def get_initial_params(rng, model):
  """Returns randomly initialized parameters."""
  token_ids = jnp.ones((2, 3), jnp.int32)
  lengths = jnp.ones((2,), dtype=jnp.int32)
  variables = model.init(rng, token_ids, lengths, deterministic=True)
  return variables['params']


def create_train_state(rng, config: ml_collections.ConfigDict, model):
  """Create initial training state."""
  params = get_initial_params(rng, model)
  tx = optax.chain(
      optax.sgd(learning_rate=config.learning_rate, momentum=config.momentum),
      optax.additive_weight_decay(weight_decay=config.weight_decay))
  state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)
  return state


def compute_metrics(*, labels: Array, logits: Array) -> Metrics:
  """Computes the metrics, summed across the batch if a batch is provided."""
  if labels.ndim == 1:  # Prevent the labels from broadcasting over the logits.
    labels = jnp.expand_dims(labels, axis=1)
  loss = sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)
  binary_predictions = (logits >= 0.)
  binary_accuracy = jnp.equal(binary_predictions, labels)
  return Metrics(
      loss=jnp.sum(loss),
      accuracy=jnp.sum(binary_accuracy),
      count=logits.shape[0])


def model_from_config(config: ml_collections.ConfigDict):
  """Builds a text classification model from a config."""
  model = models.TextClassifier(
      embedding_size=config.embedding_size,
      hidden_size=config.hidden_size,
      vocab_size=config.vocab_size,
      output_size=config.output_size,
      dropout_rate=config.dropout_rate,
      word_dropout_rate=config.word_dropout_rate,
      unk_idx=config.unk_idx)
  return model


def train_step(
    state: TrainState,
    batch: Dict[str, Array],
    rngs: Dict[str, Any],
) -> Tuple[TrainState, Metrics]:
  """Train for a single step."""
  # Make sure to get a new RNG at every step.
  step = state.step
  rngs = {name: jax.random.fold_in(rng, step) for name, rng in rngs.items()}

  def loss_fn(params):
    variables = {'params': params}
    logits = state.apply_fn(
        variables, batch['token_ids'], batch['length'],
        deterministic=False,
        rngs=rngs)

    labels = batch['label']
    if labels.ndim == 1:
      labels = jnp.expand_dims(labels, 1)
    loss = jnp.mean(
        sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))
    return loss, logits

  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
  value, grads = grad_fn(state.params)
  (_, logits) = value

  new_state = state.apply_gradients(grads=grads)
  metrics = compute_metrics(labels=batch['label'], logits=logits)
  return new_state, metrics


def eval_step(state: TrainState, batch: Dict[str, Array],
              rngs: Dict[str, Any]) -> Metrics:
  """Evaluate for a single step. Model should be in deterministic mode."""
  variables = {'params': state.params}
  logits = state.apply_fn(
      variables, batch['token_ids'], batch['length'],
      deterministic=True,
      rngs=rngs)
  metrics = compute_metrics(labels=batch['label'], logits=logits)
  return metrics


def normalize_batch_metrics(
        batch_metrics: Sequence[Metrics]) -> Metrics:
  """Consolidates and normalizes a list of per-batch metrics dicts."""
  # Here we sum the metrics that were already summed per batch.
  total_loss = np.sum([metrics.loss for metrics in batch_metrics])
  total_accuracy = np.sum([metrics.accuracy for metrics in batch_metrics])
  total = np.sum([metrics.count for metrics in batch_metrics])
  # Divide each metric by the total number of items in the data set.
  return Metrics(
      loss=total_loss.item() / total, accuracy=total_accuracy.item() / total)


def batch_to_numpy(batch: Dict[str, tf.Tensor]) -> Dict[str, Array]:
  """Converts a batch with TF tensors to a batch of NumPy arrays."""
  # _numpy() reuses memory, does not make a copy.
  # pylint: disable=protected-access
  return jax.tree_map(lambda x: x._numpy(), batch)


def evaluate_model(
        eval_step_fn: Callable[..., Any],
        state: TrainState,
        batches: Union[Iterable[Example], tf.data.Dataset],
        epoch: int,
        rngs: Optional[Dict[str, Any]] = None
) -> Metrics:
  """Evaluate a model on a dataset."""
  batch_metrics = []
  for i, batch in enumerate(batches):
    batch = batch_to_numpy(batch)
    if rngs is not None:  # New RNG for each step.
      rngs = {name: jax.random.fold_in(rng, i) for name, rng in rngs.items()}

    metrics = eval_step_fn(state, batch, rngs)
    batch_metrics.append(metrics)

  batch_metrics = jax.device_get(batch_metrics)
  metrics = normalize_batch_metrics(batch_metrics)
  logging.info('eval  epoch %03d loss %.4f accuracy %.2f', epoch,
               metrics.loss, metrics.accuracy * 100)
  return metrics


def train_epoch(train_step_fn: Callable[..., Tuple[TrainState, Metrics]],
                state: TrainState,
                train_batches: tf.data.Dataset,
                epoch: int,
                rngs: Optional[Dict[str, Any]] = None
                ) -> Tuple[TrainState, Metrics]:
  """Train for a single epoch."""
  batch_metrics = []
  for batch in train_batches:
    batch = batch_to_numpy(batch)
    state, metrics = train_step_fn(state, batch, rngs)
    batch_metrics.append(metrics)

  # Compute the metrics for this epoch.
  batch_metrics = jax.device_get(batch_metrics)
  metrics = normalize_batch_metrics(batch_metrics)

  logging.info('train epoch %03d loss %.4f accuracy %.2f', epoch,
               metrics.loss, metrics.accuracy * 100)

  return state, metrics


def train_and_evaluate(config: ml_collections.ConfigDict,
                       workdir: str) -> TrainState:
  """Execute model training and evaluation loop.
  Args:
    config: Hyperparameter configuration for training and evaluation.
    workdir: Directory where the tensorboard summaries are written to.
  Returns:
    The final train state that includes the trained parameters.
  """
  # Prepare datasets.
  train_dataset = input_pipeline.TextDataset(
      tfds_name='glue/sst2', split='train')
  eval_dataset = input_pipeline.TextDataset(
      tfds_name='glue/sst2', split='validation')
  train_batches = train_dataset.get_bucketed_batches(
      config.batch_size,
      config.bucket_size,
      max_input_length=config.max_input_length,
      drop_remainder=True,
      shuffle=True,
      shuffle_seed=config.seed)
  eval_batches = eval_dataset.get_batches(batch_size=config.batch_size)

  # Keep track of vocab size in the config so that the embedder knows it.
  config.vocab_size = len(train_dataset.vocab)

  # Compile step functions.
  train_step_fn = jax.jit(train_step)
  eval_step_fn = jax.jit(eval_step)

  # Create model and a state that contains the parameters.
  rng = jax.random.PRNGKey(config.seed)
  model = model_from_config(config)
  state = create_train_state(rng, config, model)

  summary_writer = tensorboard.SummaryWriter(workdir)
  summary_writer.hparams(dict(config))

  # Main training loop.
  logging.info('Starting training...')
  for epoch in range(1, config.num_epochs + 1):

    # Train for one epoch.
    rng, epoch_rng = jax.random.split(rng)
    rngs = {'dropout': epoch_rng}
    state, train_metrics = train_epoch(
        train_step_fn, state, train_batches, epoch, rngs)

    # Evaluate current model on the validation data.
    eval_metrics = evaluate_model(eval_step_fn, state, eval_batches, epoch)

    # Write metrics to TensorBoard.
    summary_writer.scalar('train_loss', train_metrics.loss, epoch)
    summary_writer.scalar(
        'train_accuracy',
        train_metrics.accuracy * 100,
        epoch)
    summary_writer.scalar('eval_loss', eval_metrics.loss, epoch)
    summary_writer.scalar(
        'eval_accuracy',
        eval_metrics.accuracy * 100,
        epoch)

  summary_writer.flush()
  return state
#+end_src

* Introduction
** Acomplishments
The acomplishments of this Assignment are as follows:
- Doobie
- Scooby do
* Stochastic Gradient Descent: Constant and Adam Step Sizes


| Constant | Alpha | Batch Size |
|----------+-------+------------|
| Default  |   0.1 |        128 |
| Optimal  | 0.489 |         49 |

| Adam    | Alpha | Batch Size | Beta1 |  Beta2 |
|---------+-------+------------+-------+--------|
| Default |  0.01 |        128 |   0.9 |  0.999 |
| Optimal | 0.015 |         98 | 0.898 | 0.9575 |



* Report
** MNIST 
sgd_default_alpha = 0.1
sgd_default_batch = 128

sgd_optimal_alpha = 0.489
sgd_optimal_batch = 49

adam_default_alpha = 0.01
adam_default_b1 = 0.9
adam_default_b2 = 0.999
adam_default_batch = 128

adam_optimal_alpha = 0.0015
adam_optimal_b1 = 0.898
adam_optimal_b2 = 0.9575
adam_optimal_batch = 98


Global random search was used to find the optimal hyperparameters.
20 runs with for each Cosntant step and Adam. Ranges were

#+begin_export latex
\begin{figure}[htb]
\centering
% \captionbox{\label{fig:a1}}{\includegraphics[width=\figwidth\textwidth]{final/a1.png}}
% \captionbox{\label{fig:a2}}{\includegraphics[width=\figwidth\textwidth]{final/a2.png}}\\[2ex]
\end{figure}
\clearpage
#+end_export










* Appendix
** Code Listing
#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\lstinputlisting[language=Python]{FinalSrc.py}
#+end_export

