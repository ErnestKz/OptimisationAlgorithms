#+AUTHOR:Ernests Kuznecovs - 17332791
#+Date:7th February 2022
#+Title:Optimisation Algorithms - Week 2 Assignment

#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
#+end_export
* Preamble :noexport:
#+PROPERTY: header-args:python :session a1
#+PROPERTY: header-args:python+ :async yes
#+PROPERTY: header-args:python+ :eval never-export
#+PROPERTY: header-args:elisp :eval never-export
#+EXCLUDE_TAGS: noexport

#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \usepackage{minted}
# #+LaTeX_HEADER: \usepackage{subfig}
#+LaTeX_HEADER: \usepackage{subcaption}
#+LaTeX_HEADER: \usepackage[a4paper, total={7.2in, 10.2in}]{geometry}


# \usepackage{natbib}
# \usepackage[margin=0.5in]{geometry}
# \usepackage{amsmath} 
# \usepackage{graphicx}
# \usepackage{ragged2e}
# \usepackage{subcaption}
# \usepackage{grffile}
# \usepackage{cleveref}


#+begin_src elisp :results none :exports none
(setq-local org-image-actual-width '(512))
(setq-local org-confirm-babel-evaluate nil)
(setq-local org-src-preserve-indentation 't)
;; (setq-local org-export-use-babel nil)

;; (setq org-latex-listings 'minted)
(setq org-latex-listings t)

;; (setq org-latex-minted-options
;;     '(
;;       ;; ("bgcolor" "bg")
;;       ("frame" "lines")))

;; (setq org-latex-listings-options
;;     '(("basicstyle" "\\small")
;;       ("keywordstyle" "\\color{black}\\bfseries\\underbar")))

;; (setq org-latex-listings-options nil)

;; (setq org-latex-pdf-process
;;       (mapcar
;;        (lambda (s)
;;          (replace-regexp-in-string "%latex " "%latex -shell-escape " s))
;;        org-latex-pdf-process))
#+end_src


#+begin_src python :results none :exports none :tangle ./Week2Src.py
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 200
mpl.rcParams['figure.facecolor'] = '1'
import matplotlib.pyplot as plt

import numpy as np
import sympy
#+end_src

* (a) Derivatives and Finite Difference for $y(x) = x^4$
** (i) Symbolic Derivative
Using the symbolic maths library sympy, a symbol object $x$ is created, $x \in \mathbb{R}$.
Then the **4 operator is applied to the object, now it becomes the expression $x^4$.
The resulting expression can be passed to the sympy.diff function to differentiate it with respect to $x$.
Differentiating it will now give a sympy object representing $4x^3$.

# #+ATTR_LATEX: :options style=mystyle 
#+begin_src python :exports both :tangle ./Week2Src.py 
x = sympy.symbols('x', real=True)
y = x**4
dydx = sympy.diff(y,x)
print(dydx)
#+end_src

#+RESULTS[fabc0af0f6145db33746d116894eae83f31e4575]:
: 4*x**3

Using these expressions, sympy can turn them into functions that takes an argument with the sympy.lambdify function.
Effectively giving us the expressions $y(x)=x^{4}$ and $\frac{dy}{dx}(x)=4x^3$. 

#+begin_src python :exports code :results none :tangle ./Week2Src.py
y = sympy.lambdify(x, y)
dydx = sympy.lambdify(x, dydx)
#+end_src

** (ii) Finite Difference Implementation
The python function that computes the finite difference of a function:
- Inputs are:
  - $f$: the function
  - $x$: input value for the function
  - $\delta$: the perturbation 
- The finite difference can be implemented as $\frac{f(x) - f(x - \delta)}{\delta}$
-  $\frac{f(x + \delta) - f(x - \delta)}{2 * \delta}$ could be used to negate the offset (perturbation is $2*\delta$ in this case).
- Finite difference nudges a function a tiny bit in a direction and then divides by that difference to find by how much the function value changed relative to that nudge, giving the slope.
  
#+begin_src python :results none :exports code :tangle ./Week2Src.py
def finiteDiff(f, x, delta):
    return (f(x) - f(x - delta)) / delta
#+end_src

#+begin_src python :results none :exports none :tangle ./Week2Src.py
def finiteDiff(f, x, delta):
    return (f(x + delta) - f(x - delta)) / (2 * delta)
#+end_src

Fig [[fig:fd]] shows finite difference method with $\delta=0.01$ generates a curve almost identical to the symoblic one, although a slight fringe of blue is seen at $x>1$ and $x<-1$, indicating the tiny offset caused by the nudge in one direction.

#+caption: Finite Difference vs. Symbolic Derivative; for $y(x) = x^4$
#+LABEL: fig:fd
#+attr_latex: :width 8cm :options angle=0
[[file:./images_week2/finite_difference.png]]

*** Plotting Code :noexport:
#+begin_src python :exports none :results none :tangle ./Week2Src.py
def axset(ax, xrange, xoffset, yrange, yoffset):
    ax.set(xlim=(xoffset-xrange, xoffset+xrange),
           ylim=(yoffset-yrange, yoffset+yrange))
#+end_src

#+begin_src python :exports none :file ./images_week2/finite_difference.png :tangle ./Week2Src.py
xs = np.arange(-20, 20, 0.1)

ys_sym = dydx(xs)

ys_finiteDiff = []
for x in xs:
    ys_finiteDiff.append(finiteDiff(y, x, 0.01))

fig, ax = plt.subplots()
ax.set_ylabel(r'$\frac{dy}{dx}(x)$')
ax.set_xlabel(r'$x$')
ax.set_title(r'Finite Difference vs. Symbolic Derivative'  "\n" r'for $y(x) = x^4$')

ax.plot(xs, ys_sym, linewidth=2.0)
ax.plot(xs, ys_finiteDiff, linewidth=2.0)
ax.legend(("Symbolic", r'Finite Difference with $\delta = 0.01$'))
axset(ax, xrange=2, xoffset=0, yrange=20, yoffset=0)

# fig.show()

# ax.set(
#     xlim=(-3, 3),
#     ylim=(-20, 20),
#     xticks=np.arange(1, 8),
#     yticks=np.arange(1, 8),
#      )
#+end_src

** (iii) Varying $\delta$ on Finite Difference
In [[fig:fd2]], as $\delta$ increases, we can see the error getting bigger an bigger for values that are away from $x=0$. Error is bigger further away as a nudge in $x$ causes a larger change. A $\delta<0.01$, seems to produce little error even at large $x$.
#+caption: Varying $\delta$ on finite difference method.
#+LABEL: fig:fd2
#+attr_latex: :width 8cm :options angle=0
[[file:images_week2/varying_delta.png]]

*** Varying $\delta$ on Finite Difference Plotting Code            :noexport:
#+begin_src python :exports none :file ./images_week2/varying_delta.png :tangle ./Week2Src.py 
dydx = lambda x: 4 * x**3
y = lambda x: x**4

xs = np.arange(-20, 20, 0.1)

deltas = [0.001, 0.01, 0.1, 0.5, 1]
ys_dif = []
for delta in deltas:
  dif = []
  for x in xs:
      fd = finiteDiff(y, x, delta)
      ex = dydx(x)
      dif += [ex - fd]
      
  ys_dif += [(dif, delta)]

fig, ax = plt.subplots()
legend_labels = []  
for (diff, delta) in ys_dif:
    legend_labels += [r'$\delta = $' + str(delta)]
    ax.plot(xs, diff, linewidth=2.0)

ax.set_title(r'Varying $\delta$ on diffs f.d vs. sympy'  "\n" r'for $y(x) = x^4$')
ax.set_ylabel(r'sympy - f.d ')
ax.set_xlabel(r'$x$')
ax.legend(legend_labels)
# axset(ax, xrange=3, xoffset=1.5, yrange=20, yoffset=10)
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7ff0480d0e20>
[[file:./images_week2/varying_delta.png]]
:END:

* (b) Gradient Descent Optimisation Algorithm
** (i) Gradient Descent Implementation
Gradient descent (g.d) finds the $x$ that minimises some function $f(x)$ i.e g.d finds $argmin_x f(x)$.
- The implementation uses the derivative of $f(x)$ i.e $\frac{df}{dx}(x)$.
- g.d requires a starting $x$ value i.e $x_{0}$.
- For some defined number of iterations $i_{max}$, g.d iteratively adjusts $x_i$.
- One iteration approximates how to modify $x_i$ in order to move towards the minumum of $f(x)$.
- Approximating is acomplished by using $\frac{df}{dx}(x)$ to find the slope of the curve at point $x_i$, and using the slope as the local approximation for which direction relative to the point $f(x_i)$, the minimum of $f(x)$ lies.
- A step size for $x_i$ is calculated by multiplying $\frac{df}{dx}(x)$ by some scalar $\alpha$, in this case $\alpha$ is manually picked and stays constant throughout all the iterations, although the magnitude of $\frac{df}{dx}(x)$ itself may change and alter the step magnitude.
- The negative of $\frac{df}{dx}(x_i)$ guarantees an instantaneous step for $x_{i}$ in the downwards direction for $f(x_i)$. $x_{i+1} = x_{i} + step$, and the process is repeated.

#+begin_src python :results none :exports code :tangle ./Week2Src.py
def gradient_descent(df, x0, alpha=0.15, i_max=50):
    x = x0
    for k in range(i_max):
        step = alpha * -df(x)
        x = x + step
    return x
#+end_src

*** Gradient Descent Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week2Src.py
class QuadraticFn():
    def f(self, x):
        return x**2                       # function value f(x)
    
    def df(self, x):
        return x*2                        # derivative of f(x)
    
fn = QuadraticFn()

def gradDesc(fn, x0, alpha=0.15, num_iters=50):
    x = x0                                # starting point
    X = np.array([x])                     # array of x history
    F = np.array(fn.f(x))                 # array of f(x) history
    for k in range(num_iters):
        step = alpha * fn.df(x)
        x = x - step
        X = np.append(X, [x], axis=0)     # add current x to history
        F = np.append(F, fn.f(x))         # add value of current f(x) to history
    return (X,F)

def gradDesc3(f, df, x0, alpha=0.15, num_iters=50):
    x = x0                                # starting point
    X = np.array([x])                     # array of x history
    F = np.array(f(x))                 # array of f(x) history
    for k in range(num_iters):
        step = alpha * df(x)
        x = x - step
        # print(x)
        X = np.append(X, [x], axis=0)     # add current x to history
        F = np.append(F, f(x))         # add value of current f(x) to history
    return (X,F)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week2Src.py
(X, F) = gradDesc(fn, 1)
x = gradient_descent(fn.df, 1)
#+end_src

#+RESULTS:

** (ii) Visualising Gradient Descent
Gradient Descent is run with, $x_0=1$, $\alpha=0.1$, $y(x) = x^4$.
$x$ and $y(x)$ vary with each gradient descent iteration.


- Figure [[fig:fx4]] plots the function to be optimised; it is convex, but there is a very flat portion at $-0.5 < x < 0.5$. We know that the $argmin_x f(x)=0$ for this function.
- Figure [[fig:xi]] plots $x_i$ against $i$; $x_i$ decreases rapidly on the very first iteration, reaching 40% of the way to 0, but then begins to slow down rapdily, this is because the slope of the function is significantly smaller at $x>0.6$ compared to at $x=1$, and the slope keeps on decreasing at a rate of $4x^3$, which is quite rapid for a constant $\alpha$, and the slope is important in the step as $step = \alpha * slope_{x_i}$.
- Figure [[fig:yi]] plots $y$ against $i$; the majority of the optimisation happens in 2 iterations, and very little progress is made after $i=2$, it essentially comes to a flat line 3 iterations onward.
We see that $x_i$ takes longer to become a flat line than $y(x_i)$, this is because of the flat shape of the bottom of $x^4$. Once $x_i$ reaches the bottom, $x_i$ itself can still move a bit, but will not have a equally proportional impact on $y(x_i)$.




#+begin_export latex
%\begin{figure}%
%    \centering
%    \subfloat[\centering label 1]{{\includegraphics[width=5cm]{images_week2/x_4.png} }}%
%    \qquad
%    \subfloat[\centering label 2]{{\includegraphics[width=5cm]{images_week2/x_i.png} }}%
%    \caption{2 Figures side by side}%
%    \label{fig:example}%
%\end{figure}
%
%\begin{figure}
%\centering
%\subcaptionbox{Subcaption A}{\includegraphics[width=0.40\textwidth]{images_week2/x_4.png}}%
%\hfill
%\subcaptionbox{Subcaption B}{\includegraphics[width=0.40\textwidth]{images_week2/x_4.png}}%
%\hfill
%\subcaptionbox{Subcaption C}{\includegraphics[width=0.40\textwidth]{images_week2/x_4.png}}%
%% \hfill
%% \subcaptionbox{Subcaption D}{\includegraphics[width=0.20\textwidth]{images_week2/x_4.png}}%
%\caption{Caption}
%\end{figure}

\begin{figure}[h]
\centering
\parbox{.9\textwidth}{
            \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 1}
        \end{subfigure}
        \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 2}
        \end{subfigure}\\
        \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 3}
        \end{subfigure}
        \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 4}
        \end{subfigure}
        }
     \caption{Simulation of model described by ODE1}\label{reduced}

\parbox{.9\textwidth}{
            \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 1}
        \end{subfigure}
        \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 2}
        \end{subfigure}\\
        \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 3}
        \end{subfigure}
        \begin{subfigure}{.475\linewidth}
                \includegraphics[width=\textwidth]{images_week2/x_4.png}
                \caption{Test 4}
        \end{subfigure}
        }
     \caption{Simulation of model described by ODE1}\label{reduced}     
\end{figure}

#+end_export


#+caption: Function to be Optimised
#+LABEL: fig:fx4
#+ATTR_LATEX: :height 0.2\textwidth :center
[[file:images_week2/x_4.png]]

#+caption: $x$ against $i$
#+LABEL: fig:xi
#+attr_latex: :height 0.2\textwidth :center
[[file:images_week2/x_i.png]]

#+caption: $y(x_i)$ against $i$
#+LABEL: fig:yi
#+attr_latex: :height 0.2\textwidth :center
[[file:images_week2/y_i.png]]

#+caption: Varying $x_0$, plotting $x$
#+LABEL: fig:varx0x
#+attr_latex: :width 8cm :float wrap
[[file:./images_week2/rangex0_2d_x_i.png]]

#+caption: Varying $x_0$, plotting y(x)
#+LABEL: fig:varx0y
#+attr_latex: :width 8cm :float wrap
[[file:./images_week2/rangex0_2d_y_i.png]]

#+caption: Varying $x_0$, plotting y(x), non-convergance
#+LABEL: fig:varx0ynon
#+attr_latex: :width 8cm :float wrap
[[file:./images_week2/rangex0_2d_y_i_non.png]]


#+caption: Varying $\alpha$, plotting x
#+LABEL: fig:varax
#+attr_latex: :width 8cm :float wrap
[[file:./images_week2/rangealpha_2d_x_i.png]]

#+caption: Varying $\alpha$, plotting (y)
#+LABEL: fig:varay
#+attr_latex: :width 8cm :float wrap
[[file:./images_week2/rangealpha_2d_y_i.png]]

*** Plotting Code :noexport:
**** Plotting $f(x)$ against $x$
#+begin_src python :exports none :file ./images_week2/x_4.png :tangle ./Week2Src.py
xs = np.arange(-20, 20, 0.1)

ys = dydx(xs)
ys = y(xs)

fig, ax = plt.subplots()
ax.set_ylabel(r'$y(x)$')
ax.set_xlabel(r'$x$')

ax.set_title(r'Function to be Optimised')
ax.plot(xs, ys, linewidth=2.0)
ax.plot(1, y(1), 'go')
ax.legend(("$y(x) = x^4$", r'$x_i = 1$'))

# ax.axvline(x=1, color='k', linestyle='--')
axset(ax, xrange=3, xoffset=0, yrange=1.5, yoffset=1.4)
#+end_src

#+RESULTS:
[[file:./images_week2/x_4.png]]

**** Plotting $f(x_i)$ against $i$
#+begin_src python :exports none :file ./images_week2/y_i.png :tangle ./Week2Src.py
(_, F) = gradDesc3(y, dydx, x0=1, alpha=0.1)
iters = np.arange(0, len(F))

fig, ax = plt.subplots()
ax.set_ylabel(r'$y(x_{i})$')
ax.set_xlabel(r'$i$')
ax.set_title(r'Gradient Descent; function value vs. iteration' "\n"
              r'$x_0=1, \alpha=0.1 , y(x) = x^4$',)
ax.plot(iters, F, linewidth=2.0)
ax.axvline(x=2, color='k', linestyle='--')

ax.legend((r'$y(x_{i})$ where $x_i=$ value of x at iteration $i$', r'$i=2$', ))
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7ff0683b39a0>
[[file:./images_week2/y_i.png]]
:END:

#+begin_src python :exports none :file ./images_week2/logy_i.png :tangle ./Week2Src.py
(_, F) = gradDesc3(y, dydx, x0=1, alpha=0.1)
iters = np.arange(0, len(F))

fig, ax = plt.subplots()
ax.set_ylabel(r'$y(x_{i})$')
ax.set_xlabel(r'$i$')
ax.set_title(r'Gradient Descent; function value vs. iteration; log scale' "\n"
              r'$x_0=1, \alpha=0.1 , y(x) = x^4$',)

ax.semilogy(iters, F, linewidth=2.0)
ax.legend((r'$y(x_{i})$ where $x_i=$ value of x at iteration $i$',))
#+end_src

**** Plotting $x_i$ against $i$
#+begin_src python :exports none :file ./images_week2/x_i.png :tangle ./Week2Src.py
(X, _) = gradDesc3(lambda x : x**4, lambda x : 4*x**3, x0=1, alpha=0.1)
iters = np.arange(0, len(X))

fig, ax = plt.subplots()
ax.set_ylabel(r'$x_i$')
ax.set_xlabel(r'$i$')
ax.set_title(r'$x$ Value at Beginning of each Iteration' "\n"
             r'$x_0=1, \alpha=0.1 , y(x) = x^4$',)
ax.axvline(x=2, color='k', linestyle='--')
ax.plot(iters, X, linewidth=2.0)

ax.legend((r'$i=2$', r'$x_{i}$ = value of x at iteration $i$',))
#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7ff068531250>
[[file:./images_week2/x_i.png]]
:END:

** (iii) Varying Step Size $\alpha$ and $x_0$



- Varying $x_0$
  - Plotting x - Fig. [[fig:varx0x]]: We can see that $x_0>2.236068$ would lead to an explosive non-convergance; it keeps jumping over to the other side of the curve, higher than what it was before. This is because the combination of the slope at $x>2.236068$ is too high in magnitude for the combination with $alpha=0.1$ and therefore results in too large of a step size. $x_0=2$ jumps over to the other side, but not higher than it was before and still manages to converge. Once the $x_i$ reaches within $-0.5<x<0.5$, the size of the slope is tiny relative to the $\alpha$, and essentially stops making progress.
  - Plotting y that converge - Fig. [[fig:varx0y]]: We see that even though $x_i$ dont converge on the same point for different $x_0$, they all converge on paractically the same $y$ value, and all of them within only 2 iterations.
  - Plotting y that doesn't converge - Fig. [[fig:varx0ynon]]: We can see that $x_0>2.236068$ will not converge, the function value keeps increasing due to the larger and larger jumps to each side of the convex function.

- Varying $\alpha$
  - Plotting x - Fig. [[fig:varax]]: An $\alpha>0.5$ would lead to an explosive non convergence, as it would cause jumps to the other side to a higher y value. Rest of the $\alpha$ converge, but it seems like the very first jump determines where its going to get stuck in the flat region.
  - Plotting y - Fig. [[fig:varay]]: $\alpha>0.5$ shows non-convergance, and the rest of the $\alpha 's$ converge closely to each other. $\alpha=0.1$ makes keeps making progress even after 5 iterations in, seems like it's the nature of the rapidly flattening function rather than a small constant $\alpha$ that causes the slowdown of the convergance.
Both $x_0$ and $\alpha$ cause un-forgiving explosions if not chosen small enough, but as long as the first step size is small enough, they converge to practically the same y value. The functions rapidly decreasing slope, rather than the chosen constant $\alpha$ value, is what causes the quicksand behaviour towards the minimum, a small alpha will allow for more flexible placement of $x_i$ / $x_0$ as it'll be less likely to shoot off exponentially, while still being able to converge. But this is just the behavoiur of $x^4$.
*** Varying $\alpha$ and $x_0$ - Plotting Code                     :noexport:
**** Plotting $x_i$ varying $x_0$
#+begin_src python :exports none :file ./images_week2/rangex0_3d_x_i.png :tangle ./Week2Src.py
# (X, _) = gradDesc3(y, dydx, x0=1, alpha=0.1)   # given a range of alphas, give back corresponding dimensions of answers, same for x0s
# perhaps it gives back objects that describe the shape of the output in detail, perhaps what dimension represents what, and how many there are

x0s = np.arange(0.1, 2, 0.1)
num_iters = 50

Xs = np.array([])
for x0 in x0s:
    (X, _) = gradDesc3(lambda x : x**4, lambda x : 4*x**3, x0=x0, alpha=0.1, num_iters=num_iters)
    if len(Xs) > 0:
        Xs = np.append(Xs, [X],  axis=0)
    else:
        Xs = np.array([X])

# fig, ax = plt.subplots()
# ax.set_ylabel(r'$x_i$')
# ax.set_xlabel(r'$i$')
# print(num_iters)

# print(Xs.shape)
# 0th index is x0 = 1.7
# [0,0] (x0=0.1,i=0)
# [0,1] (x0=0.1,i=1) 2 params input, Xs is the output

# [1,0] (x0=0.2,i=1)
# [1,1] (x0=0.2,i=1) 2 params input, Xs is the output

# indexes of inputs must correspond to position of output
        
itersY, x0sX = np.meshgrid(np.arange(num_iters+1), x0s)
# print(x0sX)
# print(itersY)
# print(Xs)

fig = plt.figure()
ax = plt.axes(projection='3d')
# ax.contour3D(x0sX, itersY, Xs, 100, cmap='binary')
ax.plot_surface(x0sX, itersY, Xs, rstride=1, cstride=1,
                cmap='viridis', edgecolor='none')
ax.view_init(12, 75)
# ax.view_init(12, 120)
ax.view_init(12, 30)
# ax.view_init(0, 0)

ax.set_xlabel(r'$x_0$')
ax.set_ylabel(r'$i$')
ax.set_zlabel(r'$x_i$')

# looks like i get slow on these kinds of problems
# probably practice will help
# and perhaps doing going slowly through them and
# understanding them will help
#+end_src

#+begin_src python :exports none :file ./images_week2/rangex0_2d_x_i.png :tangle ./Week2Src.py
x0s = [0.1, 0.5, 1, 1.5, 2, 2.236068]
# 2.23607
num_iters = 11

Xs = np.array([])
for x0 in x0s:
    (X, _) = gradDesc3(lambda x : x**4,
                       lambda x : 4*x**3,
                       x0=x0,
                       alpha=0.1,
                       num_iters=num_iters)
    if len(Xs) > 0:
        Xs = np.append(Xs, [(X,x0)],  axis=0)
    else:
        Xs = np.array([(X, x0)])

fig, ax = plt.subplots()
ax.set_ylabel(r'$x_i$')
ax.set_xlabel(r'$i$')
ax.set_title(r'Gradient Descent; $x_i$ vs. iteration; Varying $x_0$' "\n"
              r'$ \alpha=0.1 , y(x) = x^4$',)
legend_labels = []
for (X, x0) in Xs:
    ax.plot(range(num_iters+1), X, linewidth=2.0)
    legend_labels += [(r' $x_{0}$ = ' + str(x0))]
ax.legend(legend_labels)
#+end_src

**** Plotting $x_i$ varying $\alpha$
#+begin_src python :exports none :file ./images_week2/rangealpha_2d_x_i.png :tangle ./Week2Src.py
alphas = [0.1, 0.2, 0.3, 0.4, 0.5]
num_iters = 10

Xs = np.array([])
for alpha in alphas:
    (X, _) = gradDesc3(lambda x : x**4,
                       lambda x : 4*x**3,
                       x0=1,
                       alpha=alpha,
                       num_iters=num_iters)
    if len(Xs) > 0:
        Xs = np.append(Xs, [(X,alpha)],  axis=0)
    else:
        Xs = np.array([(X, alpha)])

fig, ax = plt.subplots()
ax.set_ylabel(r'$x_i$')
ax.set_xlabel(r'$i$')
ax.set_title(r'Gradient Descent; $x_i$ vs. iteration; Varying $\alpha$' "\n"
             r'$ x_0=1 , y(x) = x^4$',)
legend_labels = []
for (X, alpha) in Xs:
    ax.plot(range(num_iters+1), X, linewidth=2.0)
    legend_labels += [(r' $\alpha$ = ' + str(alpha))]
ax.legend(legend_labels)
#+end_src

**** Plotting $y(x_i)$ varying $x_0$

#+begin_src python :exports none :file ./images_week2/rangex0_2d_y_i.png :tangle ./Week2Src.py
x0s = [0.1, 0.5, 1, 1.5, 2]
num_iters = 4

Ys = np.array([])
for x0 in x0s:
    (_, Y) = gradDesc3(lambda x : x**4,
                       lambda x : 4*x**3,
                       x0=x0,
                       alpha=0.1,
                       num_iters=num_iters)
    if len(Ys) > 0:
        Ys = np.append(Ys, [(Y,x0)],  axis=0)
    else:
        Ys = np.array([(Y, x0)])

fig, ax = plt.subplots()
ax.set_ylabel(r'$y(x_i)$')
ax.set_xlabel(r'$i$')
ax.set_title(r'Gradient Descent; $y(x_i)$ vs. iteration; Varying $x_0$' "\n"
              r'$ \alpha=0.1 , y(x) = x^4$',)
legend_labels = []
for (Y, x0) in Ys:
    ax.plot(range(num_iters+1), Y, linewidth=2.0)
    legend_labels += [(r' $x_{0}$ = ' + str(x0))]
ax.legend(legend_labels)
#+end_src

#+begin_src python :exports none :file ./images_week2/rangex0_2d_y_i_non.png :tangle ./Week2Src.py
x0s = [0.1, 0.5, 1, 1.5, 2, 2.236068]
num_iters = 11

Ys = np.array([])
for x0 in x0s:
    (_, Y) = gradDesc3(lambda x : x**4,
                       lambda x : 4*x**3,
                       x0=x0,
                       alpha=0.1,
                       num_iters=num_iters)
    if len(Ys) > 0:
        Ys = np.append(Ys, [(Y,x0)],  axis=0)
    else:
        Ys = np.array([(Y, x0)])

fig, ax = plt.subplots()
ax.set_ylabel(r'$y(x_i)$')
ax.set_xlabel(r'$i$')
ax.set_title(r'Gradient Descent; $y(x_i)$ vs. iteration; Varying $x_0$; non-convergance' "\n"
              r'$ \alpha=0.1 , y(x) = x^4$',)
legend_labels = []
for (Y, x0) in Ys:
    ax.plot(range(num_iters+1), Y, linewidth=2.0)
    legend_labels += [(r' $x_{0}$ = ' + str(x0))]
ax.legend(legend_labels)
#+end_src

#+begin_src python :exports none :file ./images_week2/rangex0_2d_log_y_i.png :tangle ./Week2Src.py
x0s = [0.1, 0.5, 1, 1.5, 2]
num_iters = 12

Ys = np.array([])
for x0 in x0s:
    (_, Y) = gradDesc3(lambda x : x**4,
                       lambda x : 4*x**3,
                       x0=x0,
                       alpha=0.1,
                       num_iters=num_iters)
    if len(Ys) > 0:
        Ys = np.append(Ys, [(Y,x0)],  axis=0)
    else:
        Ys = np.array([(Y, x0)])

fig, ax = plt.subplots()
ax.set_ylabel(r'$x_i$')
ax.set_xlabel(r'$i$')
legend_labels = []
for (Y, x0) in Ys:
    ax.semilogy(range(num_iters+1), Y, linewidth=2.0)
    legend_labels += [(r' $x_{0}$ = ' + str(x0))]
ax.legend(legend_labels)
#+end_src

**** Plotting $y(x_i)$ varying $\alpha$

#+begin_src python :exports none :file ./images_week2/rangealpha_2d_y_i.png :tangle ./Week2Src.py
alphas = [0.1, 0.2, 0.3, 0.4, 0.5]
num_iters = 6

Ys = np.array([])
for alpha in alphas:
    (_, Y) = gradDesc3(lambda x : x**4,
                       lambda x : 4*x**3,
                       x0=1,
                       alpha=alpha,
                       num_iters=num_iters)
    if len(Ys) > 0:
        Ys = np.append(Ys, [(Y,alpha)],  axis=0)
    else:
        Ys = np.array([(Y, alpha)])

fig, ax = plt.subplots()
ax.set_ylabel(r'$y(x_i)$')
ax.set_xlabel(r'$i$')
ax.set_title(r'Gradient Descent; $y(x_i)$ vs. iteration; Varying $\alpha$' "\n"
             r'$ x_0=1 , y(x) = x^4$',)
legend_labels = []
for (Y, alpha) in Ys:
    ax.plot(range(num_iters+1), Y, linewidth=2.0)
    legend_labels += [(r' $\alpha$ = ' + str(alpha))]
ax.legend(legend_labels)
#+end_src

#+RESULTS:
:RESULTS:
: /tmp/ipykernel_198847/2328935889.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
:   Ys = np.array([(Y, alpha)])
: <__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
: <matplotlib.legend.Legend at 0x7fefe9eb7a30>
[[file:./images_week2/rangealpha_2d_y_i.png]]
:END:

* (c) Optimising $y(x) = \gamma x^2$ and $y(x) = \gamma |x|$
** (i) Optimising $y(x) = \gamma x^2$
- Change to using function $y(x) = \gamma x^2$
  - How does changing gamma affect convergence

$\frac{dy}{dx}(x)=2\gamma x$

*** Code :noexport:
#+begin_src python :exports none :results none :tangle ./Week2Src.py
from jax import grad

def visualise_fn(fn, l=-10, r=10, n=1000):
    xs = np.linspace(l, r, num=n)
    y = np.array([fn(x) for x in xs])
    plt.plot(xs,y); plt.show()

def labels_fn(ax, legend, xaxis=r'$x$', yaxis=r'$y(x)$'):
    ax.set_xlabel(xaxis)
    ax.set_ylabel(yaxis)
    ax.legend(legend)
    
def visualise_fns(fns, labels_fn=labels_fn, l=-10, r=10, n=1000):
    xs = np.linspace(l, r, num=n)
    ys = []
    fig, ax = plt.subplots()
    for fn in fns:
        y = np.array([fn(x) for x in xs])
        ax.plot(xs,y)
    labels_fn(ax)

fns_gamma = (lambda fn, gammas: [(lambda x, gamma=gamma: fn(x, gamma)) for gamma in gammas])
#+end_src

#+begin_src python :exports none :results replace :tangle ./Week2Src.py
# grad by default will take the derivative of the first parameter of the function that we pass
y = lambda x, gamma: gamma * x**2
dydx = grad(y)
gammas = [0.1, 0.2, 1, 2]
# gammas = [1]

legend = [(r'$\gamma=$'+ str(gamma)) for gamma in gammas]

labels_y = lambda ax: labels_fn(ax, legend, yaxis=r'$\gamma x^2$')
labels_dy = lambda ax: labels_fn(ax, legend, yaxis=r'$2\gamma x$')

visualise_fns(fns_gamma(dydx, gammas), labels_fn=labels_dy)
visualise_fns(fns_gamma(y, gammas), labels_fn=labels_y)
# visualise_fn(lambda x: dydx(x, 1))
#+end_src

#+begin_src python :exports none :results none :tangle ./Week2Src.py
alpha = 0.1
x0 = 1
num_iters = num_iters
grad = lambda gamma : gradDesc3(f=lambda x : y(x, gamma),
                                df=lambda x : dy(x, gamma),
                                x0=x0,alpha=0.1,num_iters=num_iters)
(X,F) = grad(0.5)

 # then perform visualisations
 # The various tools to use to inspect the functions behaviour altering gamma
#+end_src

** (ii) Optimising $y(x) = \gamma |x|$
- Repeat for function $y(x) = \gamma |x|$
*** Code :noexport:

#+begin_src python :exports none :results replace :tangle ./Week2Src.py
y = lambda x, gamma: gamma * abs(x)
dydx = grad(y)

gammas = [0.1, 0.2, 1, 2]
legend = [(r'$\gamma=$'+ str(gamma)) for gamma in gammas]
labels_y = lambda ax: labels_fn(ax, legend, yaxis=r'$\gamma |x|$')
labels_dy = lambda ax: labels_fn(ax, legend, yaxis=r'$\gamma$')

visualise_fns(fns_gamma(dydx, gammas), labels_fn=labels_dy)
visualise_fns(fns_gamma(y, gammas), labels_fn=labels_y)
#+end_src

* Appendix
** Code Listing
#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\lstinputlisting[language=Python]{Week2Src.py}

%\inputminted{Python}{Week2Src.py}
#+end_export
 
