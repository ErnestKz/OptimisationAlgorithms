#+AUTHOR:Ernests Kuznecovs - 17332791 - kuznecoe@tcd.ie
#+Date:2nd March
#+Title:Optimisation Algorithms - Week 6 Assignment

#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
#+end_export
* Preamble :noexport:
#+PROPERTY: header-args:python :session a2
#+PROPERTY: header-args:python+ :async yes
#+PROPERTY: header-args:python+ :eval never-export
#+PROPERTY: header-args:elisp :eval never-export
#+EXCLUDE_TAGS: noexport

#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage[a4paper, total={6.7in, 10.5in}]{geometry}

#+LaTeX_HEADER: \usepackage{caption}
#+LaTeX_HEADER: \newcommand\figwidth{0.48}

#+begin_src elisp :results none :exports none
(use-package jupyter
  :config
  (org-babel-do-load-languages 'org-babel-load-languages '((emacs-lisp . t)
							   (python . t)
							   (jupyter . t)))
  (org-babel-jupyter-override-src-block "python")
  (add-hook 'org-babel-after-execute-hook 'org-redisplay-inline-images)
  (org-babel-do-load-languages
   'org-babel-load-languages
   '((emacs-lisp . t)
     (python . t)
     (jupyter . t))))
#+end_src

#+begin_src elisp :results none :exports none
(setq-local org-image-actual-width '(512))
(setq-local org-confirm-babel-evaluate nil)
(setq-local org-src-preserve-indentation 't)

(setq org-latex-listings t)
(setq org-latex-prefer-user-labels t)
#+end_src

* Python Imports :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 200
mpl.rcParams['figure.facecolor'] = '1'
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')
import copy
import numpy as np
#+end_src

#+begin_src python :results none :exports none :tangle ./Week6Src.py
from OptimisationAlgorithmToolkit import Algorithms
from OptimisationAlgorithmToolkit import DataType
from OptimisationAlgorithmToolkit import Plotting
from OptimisationAlgorithmToolkit import Function
import importlib
importlib.reload(Function)
importlib.reload(Algorithms)
importlib.reload(DataType)
importlib.reload(Plotting)
from OptimisationAlgorithmToolkit.Function import BatchedFunction, SymbolicFunction
from OptimisationAlgorithmToolkit.Algorithms import ConstantStep, Polyak, RMSProp, HeavyBall, Adam
from OptimisationAlgorithmToolkit.DataType import create_labels, get_titles
from OptimisationAlgorithmToolkit.Plotting import ploty, plot_contour, plot_path, plot_step_size
#+end_src
* Obtaining Functions :noexport:

#+begin_src python :results none :exports none :tangle ./Week6Src.py
import numpy as np

def generate_trainingdata(m=25):
    return np.array([0,0])+0.25*np.random.randn(m,2)

def f(x, minibatch):
    # loss function sum_{w in training data} f(x,w)
    y=0; count=0
    for w in minibatch:
        z=x-w-1
        y=y+min(12*(z[0]**2+z[1]**2), (z[0]+8)**2+(z[1]+10)**2)   
        count=count+1
    return y/count

M = generate_trainingdata()
#+end_src

*** Code Description

- generate training points $T$ with genereate_trainingdata().
- loss function $f(x, N)$ has the form:
  - $f(x, N) = \sum\limits_{w \in N} loss(x,w)$
  - $x$ is a vector with two elements, the parameters that minimise the function f.
  - $N$ is a subset/mini-batch of the training data over which to calculate loss.
    - $N$ is a set of indices that index into the training set.
      - $N$ has b elements, where b is the batch size.
      - Each iteration we pick b random indices out m, and this will be $N$
      - The loss function is calculted using these indices from $N$
      
    - Wireframe and countour plot it.
      

- Lets understand loss functions of nueral nets and regression.

*** Linear Regression/ Least Squares
- Training Data: $(x^{(i)}, y^{(i)}), i=1,...,m$
  - $y^{(i)}$ is real valued.
- Cost Function: $J(\theta) = \frac{1}{m} \sum\limits_{i=1}^{m}(\theta^Tx^{(i)} - y^{(i)})^2$
  - Cost function is quadratic in $\theta$ so expect nice convergence.
  - We are optimising $\theta$, number of dimensions depends on number of features per datapoint.
  - To add noise:
    - $y^{(i)}=\theta^{T} x^{(i)} + n^{(i)}$
    - $n^{(i)}$ normal with mean 0, std dev 0.1

  - Perhaps would be nice to write this out.
    - For some set of points

  - lets say 2 points, with 2 features
    - (x1 x2 y) (x1 x2 y) (
    - $J(\theta_1, \theta_2)=\frac{1}{2}* ((\theta_1 x_1^{(1)} + \theta_2 x_2^{(1)} - y^{(1)}})^{2} +
      (\theta_1 x_1^{(2)} + \theta_2 x_2^{(2)} - y^{(2)}})^2)$
    - $(\theta_1 x_1^{(1)} + \theta_2 x_2^{(1)} - y^{(1)}})^{2}=$ $-\theta_1 x_1^{(1)} y^{(1)} - \theta_2 x_2^{(1)} y^{(1)} + y^{(1)}^2 + \hdots$ 

- Logistic Regression with l2 and l1 regularisation.

*** Neural Networks
- Network with one hidden layer.
  
- $z_1 = f(\theta^{[1]}_{01} x_0 + \theta^{[1]}_{11} x_1 + \hdots + \theta^{[1]}_{n1} x_n)$
- $z_1 = f(\theta^{[1]}_{02} x_0 + \theta^{[1]}_{12} x_1 + \hdots + \theta^{[1]}_{n2} x_n)$
- $\hat{y} = g(\theta^{[2]}_1 z_1 + \theta^{[2]}_2 z_2)$

- Each layer has its own vector of thetas.
  - Typical choice for f and ReLu are Sigmoid.
  - For g is Sigmoid



- While I'm doing the assignment, write my thoughts in a presentable way as I go.
  - This will probably save time and effort.
*** SGD
- Use approximate derivatives $Df_{\theta_1}(\theta)$ instead of exact derivatives $\frac{\partial f}{\partial \theta_1} (\theta)$
  - $Df_{\theta_i} = \frac{\partial f}{\partial \theta_i} + noise$
  - For ML we are trying to optimise the function:
    - $J(\theta) = \frac{1}{m} \sum\limits_{i=1}^m loss(\theta, x^{(i)}, y^{(i)})$
    - $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \hdots,(x^{(m)}, y^{(m)})$ is our training data.
    - $loss()$ is a function that measures how well our predictions match the training data.
      - e.g $loss(\theta, x^{(i)}, y^{(i)}) = (\theta^T x^{(i)} - y^{(i)})$ in linear regression.
  - Derivatives:
    - $\frac{\partial J}{\partial \theta_1} (\theta) = \frac{1}{m} \sum\limits_{i=1}^{m} \frac{\partial loss}{\partial \theta_1}(\theta, x^{(i)},y^{(i)}), \frac{\partial J}{\partial \theta_2} (\theta) = \frac{1}{m} \sum\limits_{i=1}^{m} \frac{\partial loss}{\partial \theta_2}(\theta, x^{(i)},y^{(i)})$

    - Pick random sample of b points form training data
    - Let $N$ be the set of indices.
    - Then use approx derivatives:
      - $DJ_{\theta_1} (\theta) = \frac{1}{b} \sum\limits_{i \in N} \frac{\partial loss}{\partial \theta_1}(\theta, x^{(i)},y^{(i)}), DJ_{\theta_2} (\theta) = \frac{1}{b} \sum\limits_{i \in N} \frac{\partial loss}{\partial \theta_2}(\theta, x^{(i)},y^{(i)})$
	- we give it a new N every approximation, its a set of indices into the dataset.
	  - for each i in N (i stands for index)
	- when $b=m$ and $N = \{ 1,2,\hdots,m \}$ we get back exact derivatives.

	- $loss(\theta, x^{(i)}, y^{(i)}) = \frac{1}{2} * (\theta^T x^{(i)} - y^{(i)}})^2$
	- $\frac{\partial loss}{\partial \theta_k} (\theta, x^{(i)}, y^{(i)}) = (\theta^T x^{(i)} - y^{(i)}})x_k^{(i)}$
	- $DJ_{\theta_1}(\theta) = \frac{1}{5} \sum\limits_{i \in N} \frac{\partial loss}{\partial \theta_1}(\theta, x^{(i)}, y^{(i)}) = \frac{1}{5} \sum\limits_{i \in N} (\theta^T x^{(i)} - y^{(i)}})x_k^{(1)}$
	  - and so on for $\theta_2,\hdots$

	- sample with replacement after every update
	  - need to differentiate a new loss function after every iteration?

* (a) Stochastic Gradient Descent
** (i) Implementation of SGD
- Use approximate derivatives $Df_{\theta_1}(\theta)$ instead of exact derivatives $\frac{\partial f}{\partial \theta_1} (\theta)$
- For ML we are trying to optimise the function:
  - $J(\theta) = \frac{1}{m} \sum\limits_{i=1}^m loss(\theta, x^{(i)}, y^{(i)})$
  - $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \hdots,(x^{(m)}, y^{(m)})$ is our training data.
  - Real derivatives are: $\frac{\partial J}{\partial \theta_1} (\theta) = \frac{1}{m} \sum\limits_{i=1}^{m} \frac{\partial loss}{\partial \theta_1}(\theta, x^{(i)},y^{(i)}), \frac{\partial J}{\partial \theta_2} (\theta) = \frac{1}{m} \sum\limits_{i=1}^{m} \frac{\partial loss}{\partial \theta_2}(\theta, x^{(i)},y^{(i)})$

- Pick random sample of $b$ points from training data.
- Let $N$ be the set of $b$ indices.
- Then use approx derivatives:
  - $DJ_{\theta_1} (\theta) = \frac{1}{b} \sum\limits_{i \in N} \frac{\partial loss}{\partial \theta_1}(\theta, x^{(i)},y^{(i)}), DJ_{\theta_2} (\theta) = \frac{1}{b} \sum\limits_{i \in N} \frac{\partial loss}{\partial \theta_2}(\theta, x^{(i)},y^{(i)})$

- Below is an implementation using constant step size that uses all the data in the epoch rather than sampling randomly from the data each iteration.
  - The data is shuffled at the start of the "epoch" to have the effect of random sampling.
  - During the iterations the same data cant be picked twice and data won't be wasted. Batches of size $b$ are used to form the approximate derivative.
  - finite difference method is used to get the derivatives for each parital $i$.

#+begin_src python :results none :exports none :tangle ./Week6Src.py
x = np.array([1, 2]); b = 5; m = len(M); alpha=0.4; iters=50
for _ in range(iters):
    N = np.random.choice(np.arange(m), b)
    fN = lambda x: f(x, minibatch=M[N])
    DJ = np.array([finite_diff(fN, x, i) for i in range(len(x))])
    x = x - alpha * DJ
#+end_src

#+begin_src python :results none :exports code :tangle ./Week6Src.py
x = np.array([1, 2]); b = 5; m = len(M); alpha=0.4; iters=50
for _ in range(iters):
    np.random.shuffle(D)
    for i in np.arange(0, m, b): # 0 upto m-1, in steps of b. i.e index of each batch start
        N = np.arange(i, i + b)
        fN = lambda x: f(x, minibatch=M[N])
        DJ = np.array([finite_diff(fN, x, i) for i in range(len(x))])
        x = x - alpha * DJ
#+end_src

- Generalised version used provided in the appendix in which the function to be optimised is implemented as a Python iterator which returns a new set of approximate derivatives upon each iteration based on the batch. Each step size algorithm from previous assignment is adjusted to use this function iterator to retrieve the approximate gradients each iteration, thus giving us stochastic gradients, e.g for polyak:
  
#+begin_src python :results none :exports code :tangle ./Week6Src.py
def polyak(x0, f, f_star, eps, iters, b=None):
    fi = FunctionIterator(f, b, iters) ; f = f.function ; x = x0 ; X = [x] ; Y = [f(*x)]
    
    for fN, dfs in fi:
        fdif = f(*x) - f_star
        df_squared_sum = np.sum(np.array([df(*x)**2 for df in dfs]))
        alpha = fdif / (df_squared_sum + eps)
        x = x - alpha * np.array([df(*x) for df in dfs])

        X += [x] ; Y += [f(*x)]
    return X, Y
#+end_src

** (ii) Plotting Loss Function to Optimise
Fig \ref{fig:contour} is the contour plot. Fig \ref{fig:wireframe} is the wireframe plot.

The range of values chosen are -20 and 5. This is because for larger range it is strongly convex, but when zoomed in, there is an interesting dip to the side of the global minimum, which can be considered as a local minimum to test how the algorithm may behave coming across it.
Also the function increses rapidly beyond -20 and 5, it's already at $10^3$.

#+begin_export latex
\begin{figure}[htb]
\centering
\captionbox{\label{fig:contour}}{\includegraphics[width=\figwidth\textwidth]{images_week6/contour.png}}
\captionbox{\label{fig:wireframe}}{\includegraphics[width=\figwidth\textwidth]{images_week6/wireframe.png}}\\[2ex]
\end{figure}
% \clearpage
#+end_export
*** Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
m = len(M) ; b = m ; N = np.arange(b)
fN = lambda x1, x2: f(np.array([x1, x2]), minibatch=M[N])
x1s = np.linspace(-20, 5, 100)
x2s = np.linspace(-20, 5, 100)
X1, X2 = np.meshgrid(x1s, x2s)
Z = np.vectorize(fN)(X1, X2)
#+end_src

#+begin_src python :results none :exports none :tangle ./Week6Src.py 
from matplotlib.ticker import LogLocator
from matplotlib import cm
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/contour.png
plt.contourf(X1, X2, Z,
             locator=LogLocator(),
             cmap= plt.get_cmap('gist_earth'))
plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')
plt.title(r'Contour Plot')
plt.colorbar();
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/wireframe.png
fig = plt.figure()
ax = plt.axes(projection='3d')
# ax.contour3D(X1, X2, Z, 50, cmap='autumn')
ax.plot_wireframe(X1, X2, Z, cmap=cm.coolwarm, linewidth=0.2)
ax.view_init(10, 20)
# ax.set_title('Wireframe')
plt.xlabel(r'$x_1$')
plt.ylabel(r'$x_2$')
#+end_src

** (iii) Calculating Derivative of f

- finite diff is defined such that can specify which input parameter of the function we add the perturbation.

#+begin_src python :results replace :exports code :tangle ./Week6Src.py
def finite_diff(f, x, i, delta=0.0001):
    d = np.zeros(len(x)) ; d[i] = delta
    return (f(x) - f(x - d)) / delta
#+end_src

- We index our dataset $M$ with $N$ and create a closure in the lambda capturing the batch.
- Then we can pass the resulting function to the finite difference function.

#+begin_src python :results replace :exports code :tangle ./Week6Src.py
fN = lambda x: f(x, minibatch=M[N])
x = np.array([10, 10])
Dfx1 = finite_diff(fN, x, 0) # w.r.t x1
Dfx2 = finite_diff(fN, x, 1) # w.r.t x2
#+end_src

* (b) Optimising f
- 25 datapoints are used for function $f$
** (i) Gradient Descent with Constant Step-Size

A value of alpha of 0.085 is picked such that the gradient descent gets stuck in the local minimum, but an alpha a bit higher will cause it to escape. Perhaps the SGD will demonstrate that it will be able to escape it.

Fig \ref{fig:gdcy} is gradient decent with constant step plotting y value acorss iteration.
Fig \ref{fig:gdcc} is gradient decent with constant step on countour plot.

The GD is seen to do a form of chattering around the local minimum in Fig \ref{fig:gdcy}
#+begin_export latex
\begin{figure}[htb]
\centering
\captionbox{\label{fig:gdcy}}{\includegraphics[width=\figwidth\textwidth]{images_week6/gdcy.png}}
\captionbox{\label{fig:gdcc}}{\includegraphics[width=\figwidth\textwidth]{images_week6/gdcc.png}}\\[2ex]
\end{figure}
\clearpage
#+end_export

*** Code :noexport:

#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f, M)
o = ConstantStep.set_parameters(x0 = np.array([3,3]),
                             alpha = 0.085,
                             f = bf,
                             iters=60,
                             b = len(M)).run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/gdcy.png
ploty(copy.deepcopy(o))
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/gdcc.png
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+RESULTS:
[[file:./images_week6/gdcc.png]]

** (ii) Mini-Batch Stochastic Gradient Descent

Figs Run 1 = (\ref{fig:sgdcc}, \ref{fig:sgdcy}), Run 2 = (\ref{fig:sgdcc2}, \ref{fig:sgdcy2}), Run 3 = (\ref{fig:sgdcc3}, \ref{fig:sgdcy3}) shows the variance between runs. It can sometimes escape the local minimum, depending on if it gets the luckly batch of data that forms the function/gradient at critical times.
We can see that the algorithm can walk around at the local mimimum, and then escape. And we also see that it can get luckly and it gets the lucky batch in a timely manner to avoid the dance at the local minimum and directly step over it. Perhaps a batch causes the slope to increase and allows for the step to hop over.

In Fig \ref{fig:sgdcy} we can see that the y value gets quite close to the minimum value, but perhaps the function is quite volatile at the small bowl due its size and then it gets a batch/gradient that allows it to jump out.

In gradient descent, it is stuck chattering at a predictable, preiodic fashion, this is because the gradients stay the same for the x1 and x2's the algorithm finds itself at. Whereas the chattering seen in \ref{fig:sgdcy} is not periodic at all due to the varying approximate derivatives due to the random sampling of the data that constructs the function.

*** Code :noexport:

#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f, M)
o = ConstantStep.set_parameters(x0 = np.array([3, 3]),
                                alpha = 0.085,
                                f = bf,
                                iters=60,
                                b=[5]).run()
#+end_src

#+begin_src python :results none :exports none :tangle ./Week6Src.py 
o = ConstantStep.run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcc.png
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcy.png
ploty(copy.deepcopy(o))
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcc2.png
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcy2.png
ploty(copy.deepcopy(o))
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcc3.png
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcy3.png
ploty(copy.deepcopy(o))
#+end_src

** (iii) Varying Mini-Batch Size on SGD
Figs Run 1 = (\ref{fig:sgdccb}, \ref{fig:sgdcyb}), Run 2 = (\ref{fig:sgdccb2}, \ref{fig:sgdcyb2}), Run 3 = (\ref{fig:sgdccb3}, \ref{fig:sgdcyb3}) shows various runs with various batch sizes. Batch size of 1 almost always escapse the local minimum, batch size 25 (out of 25 data points) never escapes. While batch sizes 5 and 10 sometimes escape.

The point at which $x$ converges varies with batch size as the approximate gradient gets more and more noisy the smaller the batch size. In this case, all that is needed for it to converge to the globabl minimum is to get outside the local minimum. There is a higher chance that the algorithm will escape the local minimum when there is a lot of noise.
Smaller batch sizes will encourage escape from narrow optimum points, which is a good thing as once the model is used on unseen data, that narrow point may be subject to change. In other words, it doesn't take much variance in the data to change a narrow parts of a function. Where as a large areas of a minimum might be a place where noise won't affect it as much.

*** Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f, M)
o = ConstantStep.set_parameters(x0 = np.array([3, 3]),
                                alpha = 0.085,
                                f = bf,
                                iters=60,
                                b=[1, 5, 10, len(M)]).run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdccb.png
o = ConstantStep.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcyb.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdccb2.png
o = ConstantStep.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcyb2.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdccb3.png
o = ConstantStep.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcyb3.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

** (iv) Varying Step Size on SGD
Figs Run 1 = (\ref{fig:sgdcca}, \ref{fig:sgdcya}), Run 2 = (\ref{fig:sgdcca2}, \ref{fig:sgdcya2}), Run 3 = (\ref{fig:sgdcca3}, \ref{fig:sgdcya3})
Alpha=0.06 doesnt seems like it has a miniscule chance to walk out of the local minimum. Whereas alpha=0.085 sometimes does, and alpha=0.1 almost always does.

The smaller alphas move too slowly around the local minimum and therfore the many iterations that happen under it average out too fast into the local minimum bowl. Whereas the higher the alpha the less affected they the averaging affect, and need less successive lucky gradients to get out.

#+begin_export latex
\begin{figure}[htb]
\centering
\captionbox{\label{fig:sgdcc}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcc.png}}
\captionbox{\label{fig:sgdcy}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcy.png}}\\[2ex]
\captionbox{\label{fig:sgdcc2}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcc2.png}}
\captionbox{\label{fig:sgdcy2}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcy2.png}}\\[2ex]
\captionbox{\label{fig:sgdcc3}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcc3.png}}
\captionbox{\label{fig:sgdcy3}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcy3.png}}\\[2ex]
\end{figure}
\clearpage
\begin{figure}[htb]
\centering
\captionbox{\label{fig:sgdccb}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdccb.png}}
\captionbox{\label{fig:sgdcyb}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcyb.png}}\\[2ex]
\captionbox{\label{fig:sgdccb2}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdccb2.png}}
\captionbox{\label{fig:sgdcyb2}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcyb2.png}}\\[2ex]
\captionbox{\label{fig:sgdccb3}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdccb3.png}}
\captionbox{\label{fig:sgdcyb3}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcyb3.png}}\\[2ex]
\end{figure}
\clearpage
\begin{figure}[htb]
\centering
\captionbox{\label{fig:sgdcca}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcca.png}}
\captionbox{\label{fig:sgdcya}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcya.png}}\\[2ex]
\captionbox{\label{fig:sgdcca2}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcca2.png}}
\captionbox{\label{fig:sgdcya2}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcya2.png}}\\[2ex]
\captionbox{\label{fig:sgdcca3}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcca3.png}}
\captionbox{\label{fig:sgdcya3}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdcya3.png}}\\[2ex]
\end{figure}
\clearpage
#+end_export

*** Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f, M)
o = ConstantStep.set_parameters(x0 = np.array([3, 3]),
                                alpha =[0.06, 0.085, 0.1],
                                f = bf,
                                iters=30,
                                b=5).run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcca.png
o = ConstantStep.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcya.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcca2.png
o = ConstantStep.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcya2.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcca3.png
o = ConstantStep.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdcya3.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

* (c) Mini-Batch SGD with Different Step Calculations
- Select apropriate step size and explain choice.
- How do these different algorithms affect how f and x change over time.
- How is behaviour affected by choice of mini-batch size.
- Can use constant step size results from (b) as baseline comparison.
  
** (i) Polyak Step Size
No matter the batch size, the amount of variance between runs on the output is very high on polyak is very high. A lot of times it fails to escape local minimum. Opposite to constant step, polyak runs seemed to have a high variance with higher b. Figs \ref{fig:sgdpc} \ref{fig:sgdpy}
  
*** Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f, M)
o = Polyak.set_parameters(x0 = np.array([3, 3]),
                          f = bf,
                          iters=20,
                          f_star=0,
                          eps=1e-5,
                          b=[1, 5, 24, 25]
                          ).run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdpc.png
o = Polyak.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+RESULTS:
[[file:./images_week6/sgdpc.png]]

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdpy.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

#+RESULTS:
:RESULTS:
: []
[[file:./images_week6/sgdpy.png]]
:END:

*** Boxplot experiment :noexport:

#+begin_src python :results none :exports none :tangle ./Week6Src.py 
b1 = []
b5 = []
b24 = []
b25 = []
for i in range(50):
    o = copy.deepcopy(Polyak.run())
    b1 += [o[0]['Y'][-1]]
    b5 += [o[1]['Y'][-1]]
    b24 += [o[2]['Y'][-1]]
    b25 += [o[3]['Y'][-1]]
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py
print(max(b25))
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py
plt.boxplot([b1, b5, b24, b25])
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py
o = Polyak.run()
# print(o[0]['X'])
print(o[0]['Y'])
print(o)
#+end_src

** (ii) RMPSProp
Beta and alpha were picked such that b = len(M) would get stuck in the local minimum. Having b 1 and 5 allowed b=1 to escape the local minimum, but RMSProp would run out of steam when it got to going down the big bowl. Figs \ref{fig:sgdrmsc} \ref{fig:sgdrmsy}
*** Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f, M)
o = RMSProp.set_parameters(x0 = np.array([3, 3]),
                           f = bf,
                           iters=60,
                           alpha0=[0.08],
                           beta=[0.9],
                           eps=0.0001,
                           b=[1, 5, 25]).run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdrmsc.png
o = RMSProp.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdrmsy.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

** (iii) Heavy Ball
A highish beta was picked to see the momentum in action and an alpha that caused b = len(M) to just escape the local minimum. Lowering b e.g 1 and 5, in a lot of runs messes up the heavy balls ability to escape the local minimum, i.e it interrupted the momentum of the heavy ball. Heavy ball can be seen jerked around the local minimum due to the noise and sharp changes, in effect, the noise negating the momentum. Figs \ref{fig:sgdhbc} \ref{fig:sgdhby}
*** Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f,M)
o = HeavyBall.set_parameters(x0 = np.array([3, 3]),
                           f = bf,
                           iters=60,
                           alpha=[0.08],
                           beta=0.9,
                           b=[1, 5, 25]).run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdhbc.png
o = HeavyBall.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+RESULTS:
[[file:./images_week6/sgdhbc.png]]

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdhby.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

#+RESULTS:
:RESULTS:
: []
[[file:./images_week6/sgdhby.png]]
:END:

** (iv) Adam
beta1=0.99, and beta2=0.98 is set to accentuate the components of Adam. b can be seen to have quite a negligable effect. Even though for alpha=2, the full data batch size would be almost escaping, adding noise still doesn't allow it to escape, seems like the 2 averaging components of Adam are negating the effects of the noise of the function. We can see that the b=5 and b=25 are going side by side even on greatly different alphas. Figs \ref{fig:sgdac} \ref{fig:sgday}

#+begin_export latex
\begin{figure}[htb]
\centering
\captionbox{\label{fig:sgdpc}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdpc.png}}
\captionbox{\label{fig:sgdpy}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdpy.png}}\\[2ex]
\end{figure}
\clearpage
\begin{figure}[htb]
\centering
\captionbox{\label{fig:sgdrmsc}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdrmsc.png}}
\captionbox{\label{fig:sgdrmsy}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdrmsy.png}}\\[2ex]
\captionbox{\label{fig:sgdhbc}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdhbc.png}}
\captionbox{\label{fig:sgdhby}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdhby.png}}\\[2ex]
\captionbox{\label{fig:sgdac}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgdac.png}}
\captionbox{\label{fig:sgday}}{\includegraphics[width=\figwidth\textwidth]{images_week6/sgday.png}}\\[2ex]
\end{figure}
\clearpage
#+end_export

*** Code :noexport:
#+begin_src python :results none :exports none :tangle ./Week6Src.py
bf = BatchedFunction(f, M)
o = Adam.set_parameters(x0 = np.array([3, 3]),
                        f = bf,
                        iters=60,
                        alpha=[2, 20],
                        beta1=0.99,
                        beta2=0.98,
                        eps=0.0001,
                        b=[5, 25]).run()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgdac.png
o = Adam.run()
x1s = np.linspace(-20, 5, 50)
x2s = np.linspace(-20, 5, 50)
plot_contour(copy.deepcopy(o), x1s, x2s, log=True)
#+end_src

#+RESULTS:
[[file:./images_week6/sgdac.png]]

#+begin_src python :results replace :exports none :tangle ./Week6Src.py :file ./images_week6/sgday.png
ploty(copy.deepcopy(o)).semilogy()
#+end_src

#+RESULTS:
:RESULTS:
: []
[[file:./images_week6/sgday.png]]
:END:

* Appendix
** Code Listing
#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\lstinputlisting[language=Python]{Week6Src.py}
\lstinputlisting[language=Python]{./OptimisationAlgorithmToolkit/Algorithms.py}
\lstinputlisting[language=Python]{./OptimisationAlgorithmToolkit/DataType.py}
\lstinputlisting[language=Python]{./OptimisationAlgorithmToolkit/Function.py}
\lstinputlisting[language=Python]{./OptimisationAlgorithmToolkit/Plotting.py}
\lstinputlisting[language=Python]{./OptimisationAlgorithmToolkit/__init__.py}
%\inputminted{Python}{Week2Src.py}
#+end_export
