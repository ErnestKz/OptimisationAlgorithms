#+AUTHOR:Ernests Kuznecovs - 17332791 - kuznecoe@tcd.ie
#+Date:16th February 2022
#+Title:Optimisation Algorithms - Week 4 Assignment

#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
#+end_export

* Preamble :noexport:
#+PROPERTY: header-args:python :session a2
#+PROPERTY: header-args:python+ :async yes
#+PROPERTY: header-args:python+ :eval never-export
#+PROPERTY: header-args:elisp :eval never-export
#+EXCLUDE_TAGS: noexport

#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage[a4paper, total={6.7in, 10.5in}]{geometry}

#+LaTeX_HEADER: \usepackage{caption}
#+LaTeX_HEADER: \newcommand\figwidth{0.48}

#+begin_src elisp :results none :exports none
(use-package jupyter
  :config
  (org-babel-do-load-languages 'org-babel-load-languages '((emacs-lisp . t)
							   (python . t)
							   (jupyter . t)))
  (org-babel-jupyter-override-src-block "python")
  (add-hook 'org-babel-after-execute-hook 'org-redisplay-inline-images)
  (org-babel-do-load-languages
   'org-babel-load-languages
   '((emacs-lisp . t)
     (python . t)
     (jupyter . t))))
#+end_src

#+begin_src elisp :results none :exports none
(setq-local org-image-actual-width '(512))
(setq-local org-confirm-babel-evaluate nil)
(setq-local org-src-preserve-indentation 't)

(setq org-latex-listings t)
(setq org-latex-prefer-user-labels t)
#+end_src

* Python Imports :noexport:
#+begin_src python :results none :exports none :tangle ./Week4Src.py
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 200
mpl.rcParams['figure.facecolor'] = '1'
import matplotlib.pyplot as plt
plt.style.use('seaborn-white')

import numpy as np
#+end_src

#+begin_src python :results none :exports none :tangle ./Week4Src.py
# import OptimisationAlgorithmToolkit
from OptimisationAlgorithmToolkit.Function import OptimisableFunction
from OptimisationAlgorithmToolkit import Algorithms
from OptimisationAlgorithmToolkit import DataType
from OptimisationAlgorithmToolkit import Plotting
import importlib
importlib.reload(Algorithms)
importlib.reload(DataType)
importlib.reload(Plotting)
from OptimisationAlgorithmToolkit.Algorithms import Polyak, Adam, HeavyBall, RMSProp, Adagrad, ConstantStep
from OptimisationAlgorithmToolkit.DataType import create_labels, get_titles
from OptimisationAlgorithmToolkit.Plotting import ploty, plot_contour
#+end_src

* Obtaining Functions
$\frac{\partial f_1}{\partial x_{1}}=12 \left(x_{1} - 9\right)^{3}, \frac{\partial f_1}{\partial x_{2}}=10 x_{2} - 90$
$f_1(x_{1},x_{2}) = 3 \left(x_{1} - 9\right)^{4} + 5 \left(x_{2} - 9\right)^{2}$

- $f_2(x, y) = 5 \left|{y - 9}\right| + \max\left(0, x - 9\right)$
  - $\frac{df_{2}}{dx}(x,y) = \theta\left(x - 9\right)$ - this is a Heaviside function
  - $\frac{df_{2}}{dy}(x,y) = 5 \operatorname{sign}{\left(y - 9 \right)}$
** Code :noexport:
*** Sympy
#+begin_src python :results none :exports none :tangle ./Week4Src.py
from sympy import symbols, Max, Abs

x1, x2 = symbols('x1 x2', real=True)
sym_f1 = 3 * (x1-9)**4 + 5 * (x2-9)**2
f1 = OptimisableFunction(sym_f1, [x1, x2], "f_1")

sym_f2 = Max(x1-9 ,0) + 5 * Abs(x2-9)
f2 = OptimisableFunction(sym_f2, [x1, x2], "f_2")

x = symbols('x', real=True)
sym_f_quadratic = x**2
f_quadratic = OptimisableFunction(sym_f_quadratic, [x], "f_q")
#+end_src
*** Contour Plot

#+begin_src python :results replace :exports code :tangle ./Week4Src.py
from matplotlib.ticker import LogLocator

l = np.linspace(-3, 19, 40)
l2 = np.linspace(-100, 100, 40)
l2 = l
x1s = l
x2s = l2
X1, X2 = np.meshgrid(x1s, x2s)
Z = np.vectorize(f1.function)(X1, X2)
plt.contourf(X1, X2, Z, locator=LogLocator(), cmap='RdGy')
plt.colorbar();
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/30bbcd3d6614ceef34a916c3ab92396fe6402800.png]]

#+begin_src python :results replace :exports code :tangle ./Week4Src.py
l = np.linspace(-10, 40, 100)
x1s = l
x2s = l
X1, X2 = np.meshgrid(x1s, x2s)
Z = np.vectorize(f2.function)(X1, X2)
# plt.contour(X1, X2, Z, cmap='RdGy')
plt.contourf(X1, X2, Z, cmap='RdGy')
plt.colorbar();
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/5efb816825fa613a8550f9e6bf030ee40080e736.png]]

* (a) Implementing Optimisation Aglorithms
** Polyak Step Size
- $\alpha = \frac{f(x) - f^*}{\Delta f(x)^T \Delta f(x) + \epsilon}$
- $x$ is a vector
- $[\frac{\partial f}{\partial x_1}(x), \frac{\partial f}{\partial x_2} (x), \ldots, \frac{\partial f}{\partial x_n}(x)] = \Delta f(x)$
- $\Delta f(x)^{T} \Delta f(x) = \sum\limits_{i=1}^{n} \frac{\partial f}{\partial x_i} (x)^2$
** Adagrad step

$\alpha_x = \frac{a_0}{\sqrt{ \frac{df}{dx} (\vec{x}_0)^2 + \frac{df}{dx}(\vec{x}_1)^2 + \hdots + \frac{df}{dx}(\vec{x}_{t-1})^{2}} + \epsilon} = \frac{\alpha_0}{\sqrt{\sum\limits_{i=1}^{t-1} \frac{df}{dx}(x_i)^2}+\epsilon}$
$t=1$
$\alpha_1 = \frac{a_0}{\sqrt{ \frac{df}{dx_1} (\vec{x}_0)^2} + \epsilon}$
$\alpha_2 = \frac{a_0}{\sqrt{ \frac{df}{dx_2} (\vec{x}_0)^2} + \epsilon}$

$t=2$
$\alpha_1 = \frac{a_0}{\sqrt{ \frac{df}{dx_1} (\vec{x}_0)^2 + \frac{df}{dx_2}(\vec{x}_1)^2}+ \epsilon}$
$\alpha_2 = \frac{a_0}{\sqrt{ \frac{df}{d\vec{x}_2} (\vec{x}_0)^2 + \frac{df}{dx_2}(\vec{x}_1)^2}  + \epsilon} }$

so we can just hold $\frac{df}{dx_1} (\vec{x}_0)^2 + \frac{df}{dx_2}(\vec{x}_1)^2$ bit in a variable
and then sum $\frac{df}{dx_3}(\vec{x}_3)^2$ onto the variable when it comes
and we can discard $\vec{x}_0, \vec{x}_1$, as its not used anywhere else

so for multiple partials, can hold the sums in a vector
upon each iteration calculate the new partial squared for each partial(this will give us a vector) and then sum each to the according sum in the vector

** RMSProp

$a_t = \frac{ a_0 }{ \sqrt{(1 - \beta) \beta^t \frac{df}{dx}(x_0)^2 + (1 - \beta) \beta^{t-1} \frac{df}{dx}(x_1)^2 + \hdots + (1 - \beta) \frac{df}{dx} (x_{t-1})^2} + \epsilon}$
$0 < \beta \leq 1$

- We only need to keep track of the sum, as the we dont use x0 anymore.
- We can simply multiply the sum with Beta to scale it and it will have the same effect.

** Heavy Ball / Polyak Momentum

** Adam
Posible to get rid of the betas with polyak?

Adam $\approx$ RMSprop + heavy ball
$m_{t+1} = \beta_1 m_t + (1 - \beta_1) \Delta f(x_t)$ heavy ball bit
- note, there is no minimising weight on the most recent term, the term is applied only on the last ones
  
$v_{t+1} = \beta_2 v_t + (1 - \beta_2)[ \frac{\partial f}{\partial x_1} (x_t)^2, \frac{\partial f}{\partial x_2} (x_t)^{2}, \hdots , \frac{\partial f}{\partial x_n}(x_t)^2 ]$ this is rms bit, it is being summmed through each step
$\hat{m}= \frac{m_{t+1}}{(1 - \beta^t_1)}, \hat{v}= \frac{v_{t+1}}{(1 - \beta^t_2)}$
$x_{t+1} = x_{t} - \alpha [\frac{\hat{m}}{\sqrt{\hat{v_1}} + \epsilon}, \frac{\hat{m}}{\sqrt{\hat{v_2}} + \epsilon},\hdots,\frac{\hat{m}}{\sqrt{\hat{v_n}} + \epsilon}]$
$m$ is running average of gradient $\Delta f(x_t)$
$v$ is running average of square gradients

$\hat{v}_{i}$ is indexing/picking out elements from the vector of updates.


Similary to heavy ball, large step size that spans a lot of the function does well.
- It's like feeding a lot of information to the algorithm and the algorithm can take advantage of that.

$[ (1 - \beta_1)\frac{\partial f}{\partial x_1} (x_t), (1 - \beta_1)\frac{\partial f}{\partial x_2} (x_t), \hdots , (1 - \beta_1)\frac{\partial f}{\partial x_n}(x_t) ]$

$(1-\beta_{1})\Delta f(x_t) = [ (1 - \beta_1)\frac{\partial f}{\partial x_1} (x_t) + (1 - \beta_1)\frac{\partial f}{\partial x_2} (x_t) + \hdots  + (1 - \beta_1)\frac{\partial f}{\partial x_n}(x_t) ]$ ? 

* (b) Inspecting Algorithm Behaviour
- To save space plot multiple curves of function value vs iteration number on single plot.
  - step size vs iteration
  - y value vs iteration
  - contour plot
    
** (i) $\alpha$ and $\beta$ in RMSProp

*** Code :noexport:
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
print(RMSProp.all_parameters)
#+end_src

#+RESULTS:
: ('x0', 'f', 'alpha0', 'beta', 'eps', 'iters')
**** f1

#+begin_src python :results none :exports none :tangle ./Week4Src.py
# outputs = RMSProp.set_parameters(
#     x0= [0, 0],
#     f=f1,
#     iters=10,
#     alpha0=[0.05, 0.5],
#     beta=[0.2, 0.98],
#     eps=0.0001).run()

outputs = RMSProp.set_parameters(
    x0= [0, 0],
    f=f1,
    iters=10,
    alpha0=0.05,
    beta=0.98,
    eps=0.0001).run()

#+end_src

#+begin_src python :results replace :exports none :tangle ./Week4Src.py
ploty(outputs).semilogy()
#+end_src

#+RESULTS:
:RESULTS:
: []
[[file:./.ob-jupyter/5d1b5cea5692c3e2d513a3632cdc560e874ff615.png]]
:END:

#+begin_src python :results replace :exports none :tangle ./Week4Src.py
l = np.linspace(-3, 19, 40)
l2 = np.linspace(-30, 30, 40)
plot_contour(outputs, l, l2, log=True)
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/cfc7768f959dff967830e8d53aba75dd5a5a9038.png]]
**** f2
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
RMSProp.set_parameters(
    x0=[20, 20],
    f=f2,
    iters=10,
    alpha0=[0.8, 0.05],
    beta=[0.2, 0.98],
    eps=0.0001)
outputs = RMSProp.run()
ploty(outputs)
#+end_src

** (ii) $\alpha$ and $\beta$ in Heavy Ball
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
print(HeavyBall.all_parameters)
#+end_src

#+RESULTS:
: ('x0', 'f', 'alpha', 'beta', 'iters')

**** f1

#+begin_src python :results replace :exports none :tangle ./Week4Src.py
outputs = HeavyBall.set_parameters(
    x0= [0, 0],
    f=f1,
    iters=50,
    alpha=[0.0001,0.00001],
    beta=[0.98, 0.8, 0.4]).run()
v2 = HeavyBall.set_parameters(
    x0= [0, 0],
    f=f1,
    iters=50,
    alpha=[0.0001,0.00001],
    beta=[0.98, 0.8, 0.4]).run()
#+end_src

#+RESULTS:

#+begin_src python :results replace :exports none :tangle ./Week4Src.py
ploty(outputs).semilogy()
#+end_src

#+begin_src python :results replace :exports none :tangle ./Week4Src.py

#+end_src

#+begin_src python :results replace :exports none :tangle ./Week4Src.py
plot_contour(v1+v2, l, l2, log=True)
#+end_src

**** f2
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
outputs = HeavyBall.set_parameters(
    x0= [0, 0],
    f=f2,
    iters=20,
    alpha=[0.1, 0.8],
    beta=[0.8, 0.4]).run()
ploty(outputs)
#+end_src

** (iii) $\alpha$, $\beta_1$ and $\beta_2$ in Adam
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
print(Adam.all_parameters)
#+end_src

#+RESULTS:
: ('x0', 'f', 'eps', 'beta1', 'beta2', 'alpha', 'iters')

**** f1
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
outputs = Adam.set_parameters(
    x0= [0, 0],
    f=f1,
    iters=200,
    alpha=10,
    beta1=[0.90, 0.98],
    beta2=[0.9, 0.98],
    eps=1e-5).run()
ploty(outputs).semilogy()
#+end_src

**** f2
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
outputs = Adam.set_parameters(
    x0= [0, 0],
    f=f2,
    iters=200,
    alpha=1,
    beta1=[0.98, 0.8, 0.3],
    beta2=[0.98, 0.78],
    eps=1e-5).run()
ploty(outputs)
#+end_src

#+RESULTS:
:RESULTS:
: <AxesSubplot:title={'center':'Adam: x0=[0 0] iters=200 alpha=1 eps=1e-05\n'}, xlabel='$i$', ylabel='$f_2$'>
[[file:./.ob-jupyter/b8c603db78e3ff71a6299b8a5939054b9379dae3.png]]0.98
:END:

* (c) Optimising ReLu - $Max(0, x)$
#+begin_src python :results none :exports none :tangle ./Week4Src.py
x = symbols('x', real=True)
sym_f_relu = Max(0, x)
f_relu = OptimisableFunction(sym_f_relu, [x], "reLu")
#+end_src

** (i) Initial Condition $x = -1$
*** Code :noexport:
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
x_init = -1

adam_o = Adam.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=10,
    alpha=1,
    beta1=[0.98],
    beta2=[0.98],
    eps=1e-5).run()
heavyball_o = HeavyBall.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=10,
    alpha=[1],
    beta=[0.98]).run()
rmsprop_o = RMSProp.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=10,
    alpha0=[1],
    beta=[0.98],
    eps=0.0001).run()
outputs = adam_o + heavyball_o + rmsprop_o

# print(outputs)
ploty(outputs)
#+end_src

** (ii) Initial Condition $x = +1$
*** Code :noexport:
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
x_init = 1

adam_o = Adam.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=10,
    alpha=1,
    beta1=[0.98],
    beta2=[0.98],
    eps=1e-5).run()
heavyball_o = HeavyBall.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=10,
    alpha=[1],
    beta=[0.98]).run()
rmsprop_o = RMSProp.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=10,
    alpha0=[1],
    beta=[0.98],
    eps=0.0001).run()
outputs = adam_o + heavyball_o + rmsprop_o

# print(outputs)
ploty(outputs)
#+end_src

** (iii) Initial Condition $x =+100$
*** Code :noexport:
#+begin_src python :results replace :exports none :tangle ./Week4Src.py
x_init = 100
iters = 100
adam_o = Adam.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=iters,
    alpha=1,
    beta1=[0.98],
    beta2=[0.98],
    eps=1e-5).run()
heavyball_o = HeavyBall.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=iters,
    alpha=[1],
    beta=[0.98]).run()
rmsprop_o = RMSProp.set_parameters(
    x0=[x_init],
    f=f_relu,
    iters=iters,
    alpha0=[1],
    beta=[0.98],
    eps=0.0001).run()
outputs = adam_o + heavyball_o + rmsprop_o

# print(outputs)
ploty(outputs)
#+end_src

#+RESULTS:
:RESULTS:
: <AxesSubplot:title={'center':'Heavy Ball: iters=100 alpha=1 x0=[100] beta=0.98\nRMSProp: beta=0.98 iters=100 alpha0=1 x0=[100] eps=0.0001\nAdam: beta1=0.98 beta2=0.98 iters=100 alpha=1 x0=[100] eps=1e-05\n'}, xlabel='$i$', ylabel='$reLu$'>
[[file:./.ob-jupyter/0f7e4fa67bccbb61b41b57180de91ab5688d6eb8.png]]
:END:
