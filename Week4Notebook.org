#+AUTHOR:Ernests Kuznecovs - 17332791 - kuznecoe@tcd.ie
#+Date:16th February 2022
#+Title:Optimisation Algorithms - Week 4 Assignment

#+begin_export latex
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}
#+end_export

* Preamble :noexport:
#+PROPERTY: header-args:python :session a2
#+PROPERTY: header-args:python+ :async yes
#+PROPERTY: header-args:python+ :eval never-export
#+PROPERTY: header-args:elisp :eval never-export
#+EXCLUDE_TAGS: noexport

#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{xcolor}
#+LaTeX_HEADER: \usepackage{minted}
#+LaTeX_HEADER: \usepackage[a4paper, total={6.7in, 10.5in}]{geometry}

#+LaTeX_HEADER: \usepackage{caption}
#+LaTeX_HEADER: \newcommand\figwidth{0.48}

#+begin_src elisp :results none :exports none
(setq-local org-image-actual-width '(512))
(setq-local org-confirm-babel-evaluate nil)
(setq-local org-src-preserve-indentation 't)

(setq org-latex-listings t)
(setq org-latex-prefer-user-labels t)

#+end_src

#+begin_src python :results none :exports none :tangle ./Week4Src.py
import matplotlib as mpl
mpl.rcParams['figure.dpi'] = 200
mpl.rcParams['figure.facecolor'] = '1'
import matplotlib.pyplot as plt

import numpy as np
import sympy
#+end_src

* Obtaining Functions

- $f_1(x, y) = 3 \left(x - 9\right)^{4} + 5 \left(y - 9\right)^{2}$
  - $\frac{df_{1}}{dx}(x,y) = 12 \left(x - 9\right)^{3}$
  - $\frac{df_{1}}{dy}(x,y) = 10 y - 90$

- $f_2(x, y) = 5 \left|{y - 9}\right| + \max\left(0, x - 9\right)$
  - $\frac{df_{2}}{dx}(x,y) = \theta\left(x - 9\right)$ - this is a Heaviside function
  - $\frac{df_{2}}{dy}(x,y) = 5 \operatorname{sign}{\left(y - 9 \right)}$
    
* (a) Implementing Optimisation Aglorithms
** Polyak Step Size
- $\alpha = \frac{f(x) - f^*}{\Delta f(x)^T \Delta f(x) + \epsilon}$
- $x$ is a vector
- $[\frac{\partial f}{\partial x_1}(x), \frac{\partial f}{\partial x_2} (x), \ldots, \frac{\partial f}{\partial x_n}(x)] = \Delta f(x)$
- $\Delta f(x)^{T} \Delta f(x) = \sum\limits_{i=1}^{n} \frac{\partial f}{\partial x_i} (x)^2$
** Adagrad step

$\alpha_x = \frac{a_0}{\sqrt{ \frac{df}{dx} (\vec{x}_0)^2 + \frac{df}{dx}(\vec{x}_1)^2 + \hdots + \frac{df}{dx}(\vec{x}_{t-1})^{2}} + \epsilon} = \frac{\alpha_0}{\sqrt{\sum\limits_{i=1}^{t-1} \frac{df}{dx}(x_i)^2}+\epsilon}$
$t=1$
$\alpha_1 = \frac{a_0}{\sqrt{ \frac{df}{dx_1} (\vec{x}_0)^2} + \epsilon}$
$\alpha_2 = \frac{a_0}{\sqrt{ \frac{df}{dx_2} (\vec{x}_0)^2} + \epsilon}$

$t=2$
$\alpha_1 = \frac{a_0}{\sqrt{ \frac{df}{dx_1} (\vec{x}_0)^2 + \frac{df}{dx_2}(\vec{x}_1)^2}+ \epsilon}$
$\alpha_2 = \frac{a_0}{\sqrt{ \frac{df}{d\vec{x}_2} (\vec{x}_0)^2 + \frac{df}{dx_2}(\vec{x}_1)^2}  + \epsilon} }$

so we can just hold $\frac{df}{dx_1} (\vec{x}_0)^2 + \frac{df}{dx_2}(\vec{x}_1)^2$ bit in a variable
and then sum $\frac{df}{dx_3}(\vec{x}_3)^2$ onto the variable when it comes
and we can discard $\vec{x}_0, \vec{x}_1$, as its not used anywhere else

so for multiple partials, can hold the sums in a vector
upon each iteration calculate the new partial squared for each partial(this will give us a vector) and then sum each to the according sum in the vector

** RMSProp

$a_t = \frac{ a_0 }{ \sqrt{(1 - \beta) \beta^t \frac{df}{dx}(x_0)^2 + (1 - \beta) \beta^{t-1} \frac{df}{dx}(x_1)^2 + \hdots + (1 - \beta) \frac{df}{dx} (x_{t-1})^2} + \epsilon}$
$0 < \beta \leq 1$

- We only need to keep track of the sum, as the we dont use x0 anymore.
- We can simply multiply the sum with Beta to scale it and it will have the same effect.

** Heavy Ball / Polyak Momentum

** Adam
Posible to get rid of the betas with polyak?

Adam $\approx$ RMSprop + heavy ball
$m_{t+1} = \beta_1 m_t + (1 - \beta_1) \Delta f(x_t)$ heavy ball bit
- note, there is no minimising weight on the most recent term, the term is applied only on the last ones
  
$v_{t+1} = \beta_2 v_t + (1 - \beta_2)[ \frac{\partial f}{\partial x_1} (x_t)^2, \frac{\partial f}{\partial x_2} (x_t)^{2}, \hdots , \frac{\partial f}{\partial x_n}(x_t)^2 ]$ this is rms bit, it is being summmed through each step
$\hat{m}= \frac{m_{t+1}}{(1 - \beta^t_1)}, \hat{v}= \frac{v_{t+1}}{(1 - \beta^t_2)}$
$x_{t+1} = x_{t} - \alpha [\frac{\hat{m}}{\sqrt{\hat{v_1}} + \epsilon}, \frac{\hat{m}}{\sqrt{\hat{v_2}} + \epsilon},\hdots,\frac{\hat{m}}{\sqrt{\hat{v_n}} + \epsilon}]$
$m$ is running average of gradient $\Delta f(x_t)$
$v$ is running average of square gradients

$\hat{v}_{i}$ is indexing/picking out elements from the vector of updates.


Similary to heavy ball, large step size that spans a lot of the function does well.
- It's like feeding a lot of information to the algorithm and the algorithm can take advantage of that.

$[ (1 - \beta_1)\frac{\partial f}{\partial x_1} (x_t), (1 - \beta_1)\frac{\partial f}{\partial x_2} (x_t), \hdots , (1 - \beta_1)\frac{\partial f}{\partial x_n}(x_t) ]$

$(1-\beta_{1})\Delta f(x_t) = [ (1 - \beta_1)\frac{\partial f}{\partial x_1} (x_t) + (1 - \beta_1)\frac{\partial f}{\partial x_2} (x_t) + \hdots  + (1 - \beta_1)\frac{\partial f}{\partial x_n}(x_t) ]$ ? 


* (b) Optimising Functions
$\frac{\partial f_1}{\partial x_{1}}=12 \left(x_{1} - 9\right)^{3}, \frac{\partial f_1}{\partial x_{2}}=10 x_{2} - 90$
$f_1(x_{1},x_{2}) = 3 \left(x_{1} - 9\right)^{4} + 5 \left(x_{2} - 9\right)^{2}$
#+begin_src python :results replace :exports code :tangle ./Week4Src.py
# countour plot
delta = 0.2
x1s = np.arange(0, 15, delta)
x2s = np.arange(0, 15, delta)
X1, X2 = np.meshgrid(x1s, x2s)
# for each pair of the x1s and x2s, there is a y value

# Z1 = np.exp(-X1**2 - X2**2)
# Z2 = np.exp(-(X1 - 1)**2 - (X2 - 1)**2)
# Z = (Z1 - Z2) * 2

print(x1s.shape)
print(x2s.shape)
print(X1.shape)
print(X2.shape)
print(Z.shape)
from matplotlib import ticker, cm
# Z = np.vectorize(f1l)(X1, X2)
Z = np.vectorize(f1l)(X1, X2)
fig, ax = plt.subplots()
CS = ax.contourf(X1, X2, Z,
                cmap=cm.PuBu_r,
                locator=ticker.LogLocator()
                 )
# manual_locations = [
#     (-1, -1.4), (-0.62, -0.7), (-2, 0.5), (1.7, 1.2), (2.0, 1.4), (2.4, 1.7)]
ax.clabel(CS, inline=True, fontsize=10,
          # manual=manual_locations
          
          )
cbar = fig.colorbar(CS)
ax.set_title('Simplest default with labels')
#+end_src

* (c) Optimising Relu
